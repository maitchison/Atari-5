Algorithm,Score,Extra Training Data,Paper Title,Date,Game
R2D2,79.3,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,icehockey
MuZero,67.04,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,icehockey
Agent57,63.64,False,Agent57: Outperforming the Atari Human Benchmark,2020,icehockey
GDI-H3(200M frames),47.11,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,icehockey
GDI-I3,44.94,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,icehockey
MuZero (Res2 Adam),41.66,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,icehockey
UCT,39.4,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,icehockey
Ape-X,33,False,Distributed Prioritized Experience Replay,2018,icehockey
DreamerV2,26,False,Mastering Atari with Discrete World Models,2020,icehockey
FQF,17.3,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,icehockey
CGP,4,False,Evolving simple programs for playing Atari games,2018,icehockey
IMPALA (deep),3.48,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,icehockey
NoisyNet-Dueling,3,False,Noisy Networks for Exploration,2017,icehockey
Prior noop,1.3,False,Prioritized Experience Replay,2015,icehockey
Duel noop,0.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,icehockey
Prior+Duel hs,0.5,False,Deep Reinforcement Learning with Double Q-learning,2015,icehockey
IQN,0.2,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,icehockey
SARSA,-3.2,False,,,icehockey
Nature DQN,-1.6,False,,,icehockey
Gorila,-1.7,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,icehockey
DQN hs,-1.6,False,Deep Reinforcement Learning with Double Q-learning,2015,icehockey
DQN noop,-1.9,False,Deep Reinforcement Learning with Double Q-learning,2015,icehockey
Duel hs,-1.3,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,icehockey
DDQN (tuned) noop,-2.7,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,icehockey
DDQN (tuned) hs,-2.5,False,Deep Reinforcement Learning with Double Q-learning,2015,icehockey
Prior hs,-0.2,False,Prioritized Experience Replay,2015,icehockey
DDQN+Pop-Art noop,-4.1,False,Learning values across many orders of magnitude,2016,icehockey
Prior+Duel noop,-0.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,icehockey
A3C LSTM hs,-1.7,False,Asynchronous Methods for Deep Reinforcement Learning,2016,icehockey
A3C FF hs,-2.8,False,Asynchronous Methods for Deep Reinforcement Learning,2016,icehockey
A3C FF (1 day) hs,-4.7,False,Asynchronous Methods for Deep Reinforcement Learning,2016,icehockey
ES FF (1 hour) noop,-4.1,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,icehockey
C51 noop,-3.5,False,A Distributional Perspective on Reinforcement Learning,2017,icehockey
Bootstrapped DQN,-1.3,False,Deep Exploration via Bootstrapped DQN,2016,icehockey
Persistent AL,-0.25,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,icehockey
Advantage Learning,-1.24,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,icehockey
QR-DQN-1,-1.7,False,Distributional Reinforcement Learning with Quantile Regression,2017,icehockey
A2C + SIL,-2.4,False,Self-Imitation Learning,2018,icehockey
POP3D,-4.12,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,icehockey
Best Learner,-9.5,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,icehockey
Agent57,580328.14,False,Agent57: Outperforming the Atari Human Benchmark,2020,qbert
QR-DQN-1,572510,False,Distributional Reinforcement Learning with Quantile Regression,2017,qbert
R2D2,408850.0,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,qbert
IMPALA (deep),351200.12,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,qbert
Ape-X,302391.3,False,Distributed Prioritized Experience Replay,2018,qbert
A2C + SIL,104975.6,False,Self-Imitation Learning,2018,qbert
MuZero (Res2 Adam),94906.25,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,qbert
DreamerV2,94688,False,Mastering Atari with Discrete World Models,2020,qbert
MuZero,72276.00,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,qbert
GDI-H3(200M frames),28657,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,qbert
GDI-I3,27800,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,qbert
NoisyNet-Dueling,27121,False,Noisy Networks for Exploration,2017,qbert
IQN,25750,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,qbert
C51 noop,23784,False,A Distributional Perspective on Reinforcement Learning,2017,qbert
A3C LSTM hs,21307.5,False,Asynchronous Methods for Deep Reinforcement Learning,2016,qbert
Duel noop,19220.3,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,qbert
Prior+Duel noop,18760.3,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,qbert
UCT,17343.4,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,qbert
Prior noop,16256.5,False,Prioritized Experience Replay,2015,qbert
MP-EB,15805,False,Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models,2015,qbert
POP3D,15396.67,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,qbert
A3C FF hs,15148.8,False,Asynchronous Methods for Deep Reinforcement Learning,2016,qbert
Bootstrapped DQN,15092.7,False,Deep Exploration via Bootstrapped DQN,2016,qbert
DDQN (tuned) noop,15088.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,qbert
VPN,14517,False,Value Prediction Network,2017,qbert
Rational DQN Average,14436,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,qbert
Advantage Learning,14368.03,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,qbert
Duel hs,14175.8,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,qbert
MFEC,14135,False,Model-Free Episodic Control with State Aggregation,2020,qbert
Recurrent Rational DQN Average,14080,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,qbert
Prior+Duel hs,14063,False,Deep Reinforcement Learning with Double Q-learning,2015,qbert
A3C FF (1 day) hs,13752.3,False,Asynchronous Methods for Deep Reinforcement Learning,2016,qbert
DQN noop,13117.3,False,Deep Reinforcement Learning with Double Q-learning,2015,qbert
DDQN (tuned) hs,11020.8,False,Deep Reinforcement Learning with Double Q-learning,2015,qbert
Nature DQN,10596,False,,,qbert
Prior hs,9944,False,Prioritized Experience Replay,2015,qbert
DQN hs,9271.5,False,Deep Reinforcement Learning with Double Q-learning,2015,qbert
Gorila,7089.8,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,qbert
DDQN+Pop-Art noop,5236.8,False,Learning values across many orders of magnitude,2016,qbert
DQN Best,4500,False,Playing Atari with Deep Reinforcement Learning,2013,qbert
Qbert Rainbow+SEER,4123.5,False,Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings,2021,qbert
Sarsa-φ-EB,4111.8,False,Count-Based Exploration in Feature Space for Reinforcement Learning,2017,qbert
Sarsa-ε,3895.3,False,Count-Based Exploration in Feature Space for Reinforcement Learning,2017,qbert
IDVQ + DRSC + XNES,1250,False,Playing Atari with Six Neurons,2018,qbert
CURL,1225.6,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,qbert
SARSA,960.3,False,,,qbert
CGP,770,False,Evolving simple programs for playing Atari games,2018,qbert
Best Learner,613.5,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,qbert
SAC,280.5,False,Soft Actor-Critic for Discrete Action Settings,2019,qbert
MAC,243.4,False,Mean Actor Critic,2017,qbert
ES FF (1 hour) noop,147.5,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,qbert
DT,25.1,False,Decision Transformer: Reinforcement Learning via Sequence Modeling,2021,qbert
DT,106.1,False,Decision Transformer: Reinforcement Learning via Sequence Modeling,2021,pong
Duel noop,21.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,pong
ES FF (1 hour) noop,21.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,pong
IQN,21,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,pong
MuZero,21.00,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,pong
R2D2,21.0,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,pong
NoisyNet-Dueling,21,False,Noisy Networks for Exploration,2017,pong
DQN Best,21,False,Playing Atari with Deep Reinforcement Learning,2013,pong
QR-DQN-1,21,False,Distributional Reinforcement Learning with Quantile Regression,2017,pong
UCT,21,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,pong
GDI-H3,21,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,pong
GDI-H3(200M frames),21,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,pong
IMPALA (deep),20.98,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,pong
MuZero (Res2 Adam),20.95,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,pong
DDQN (tuned) noop,20.9,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,pong
Prior+Duel noop,20.9,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,pong
C51 noop,20.9,False,A Distributional Perspective on Reinforcement Learning,2017,pong
Bootstrapped DQN,20.9,False,Deep Exploration via Bootstrapped DQN,2016,pong
Ape-X,20.9,False,Distributed Prioritized Experience Replay,2018,pong
A2C + SIL,20.9,False,Self-Imitation Learning,2018,pong
Agent57,20.67,False,Agent57: Outperforming the Atari Human Benchmark,2020,pong
Prior noop,20.6,False,Prioritized Experience Replay,2015,pong
DDQN+Pop-Art noop,20.6,False,Learning values across many orders of magnitude,2016,pong
POP3D,20.5,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,pong
Discrete Latent Space World Model (VQ-VAE),20.2,False,Smaller World Models for Reinforcement Learning,2020,pong
DDRL A3C,20,False,Distributed Deep Reinforcement Learning: Learn how to play Atari games in 21 minutes,2018,pong
CGP,20,False,Evolving simple programs for playing Atari games,2018,pong
DreamerV2,20,False,Mastering Atari with Discrete World Models,2020,pong
Persistent AL,19.76,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,pong
Advantage Learning,19.66,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,pong
DQN noop,19.5,False,Deep Reinforcement Learning with Double Q-learning,2015,pong
DDQN (tuned) hs,19.1,False,Deep Reinforcement Learning with Double Q-learning,2015,pong
Nature DQN,18.9,False,,,pong
Prior hs,18.9,False,Prioritized Experience Replay,2015,pong
Duel hs,18.8,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,pong
Prior+Duel hs,18.4,False,Deep Reinforcement Learning with Double Q-learning,2015,pong
Recurrent Rational DQN Average,18.13,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,pong
Rational DQN Average,18.04,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,pong
DQN hs,18.0,False,Deep Reinforcement Learning with Double Q-learning,2015,pong
Gorila,16.7,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,pong
A3C FF (1 day) hs,11.4,False,Asynchronous Methods for Deep Reinforcement Learning,2016,pong
A3C LSTM hs,10.7,False,Asynchronous Methods for Deep Reinforcement Learning,2016,pong
MAC,10.6,False,Mean Actor Critic,2017,pong
A3C FF hs,5.6,False,Asynchronous Methods for Deep Reinforcement Learning,2016,pong
CURL,2.1,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,pong
SARSA,-17.4,False,,,pong
Best Learner,-19,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,pong
SAC,-20.98,False,Soft Actor-Critic for Discrete Action Settings,2019,pong
GDI-I3,14330,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,enduro
GDI-H3(200M frames),14300,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,enduro
C51 noop,3454.0,False,A Distributional Perspective on Reinforcement Learning,2017,enduro
MuZero,2382.44,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,enduro
R2D2,2372.7,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,enduro
Agent57,2367.71,False,Agent57: Outperforming the Atari Human Benchmark,2020,enduro
MuZero (Res2 Adam),2365.81,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,enduro
IQN,2359,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,enduro
QR-DQN-1,2355,False,Distributional Reinforcement Learning with Quantile Regression,2017,enduro
Prior+Duel noop,2306.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,enduro
Duel noop,2258.2,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,enduro
Reactor 500M,2224.2,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,enduro
Prior+Duel hs,2223.9,False,Deep Reinforcement Learning with Double Q-learning,2015,enduro
Ape-X,2177.4,False,Distributed Prioritized Experience Replay,2018,enduro
Prior noop,2093.0,False,Prioritized Experience Replay,2015,enduro
Duel hs,2077.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,enduro
NoisyNet-Dueling,2013,False,Noisy Networks for Exploration,2017,enduro
DDQN+Pop-Art noop,2002.1,False,Learning values across many orders of magnitude,2016,enduro
Prior hs,1831.0,False,Prioritized Experience Replay,2015,enduro
DreamerV2,1656,False,Mastering Atari with Discrete World Models,2020,enduro
Bootstrapped DQN,1591,False,Deep Exploration via Bootstrapped DQN,2016,enduro
Persistent AL,1343.1,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,enduro
Advantage Learning,1252.7,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,enduro
DDQN (tuned) hs,1216.6,False,Deep Reinforcement Learning with Double Q-learning,2015,enduro
DDQN (tuned) noop,1211.8,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,enduro
A2C + SIL,1205.1,False,Self-Imitation Learning,2018,enduro
Rational DQN Average,1043,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,enduro
Recurrent Rational DQN Average,957,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,enduro
DQN noop,729.0,False,Deep Reinforcement Learning with Double Q-learning,2015,enduro
DQN Best,661,False,Playing Atari with Deep Reinforcement Learning,2013,enduro
DQN hs,626.7,False,Deep Reinforcement Learning with Double Q-learning,2015,enduro
POP3D,459.85,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,enduro
VPN,382,False,Value Prediction Network,2017,enduro
Nature DQN,301.8,False,,,enduro
UCT,286.3,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,enduro
SARSA,159.4,False,,,enduro
Best Learner,129.1,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,enduro
ES FF (1 hour) noop,95.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,enduro
Gorila,71.0,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,enduro
CGP,56.8,False,Evolving simple programs for playing Atari games,2018,enduro
SAC,0.8,False,Soft Actor-Critic for Discrete Action Settings,2019,enduro
IMPALA (deep),0.00,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,enduro
A3C FF (1 day) hs,-82.2,False,Asynchronous Methods for Deep Reinforcement Learning,2016,enduro
A3C FF hs,-82.5,False,Asynchronous Methods for Deep Reinforcement Learning,2016,enduro
A3C LSTM hs,-82.5,False,Asynchronous Methods for Deep Reinforcement Learning,2016,enduro
MuZero,323417.18,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,riverraid
MuZero (Res2 Adam),171673.78,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,riverraid
Ape-X,63864.4,False,Distributed Prioritized Experience Replay,2018,riverraid
Agent57,63318.67,False,Agent57: Outperforming the Atari Human Benchmark,2020,riverraid
R2D2,45632.1,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,riverraid
IMPALA (deep),29608.05,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,riverraid
FQF,23560.7,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,riverraid
Duel noop,21162.6,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,riverraid
Prior+Duel noop,20607.6,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,riverraid
IQN,17765,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,riverraid
QR-DQN-1,17571,False,Distributional Reinforcement Learning with Quantile Regression,2017,riverraid
C51 noop,17322.0,False,A Distributional Perspective on Reinforcement Learning,2017,riverraid
Duel hs,16569.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,riverraid
Prior+Duel hs,16496.8,False,Deep Reinforcement Learning with Double Q-learning,2015,riverraid
DreamerV2,16351,False,Mastering Atari with Discrete World Models,2020,riverraid
DDQN (tuned) noop,14884.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,riverraid
Prior noop,14522.3,False,Prioritized Experience Replay,2015,riverraid
A2C + SIL,14306.1,False,Self-Imitation Learning,2018,riverraid
Bootstrapped DQN,12845,False,Deep Exploration via Bootstrapped DQN,2016,riverraid
DDQN+Pop-Art noop,12530.8,False,Learning values across many orders of magnitude,2016,riverraid
A3C FF hs,12201.8,False,Asynchronous Methods for Deep Reinforcement Learning,2016,riverraid
Prior hs,11807.2,False,Prioritized Experience Replay,2015,riverraid
DDQN (tuned) hs,10838.4,False,Deep Reinforcement Learning with Double Q-learning,2015,riverraid
Advantage Learning,10585.12,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,riverraid
A3C FF (1 day) hs,10001.2,False,Asynchronous Methods for Deep Reinforcement Learning,2016,riverraid
Nature DQN,8316.0,False,,,riverraid
POP3D,8052.23,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,riverraid
DQN noop,7377.6,False,Deep Reinforcement Learning with Double Q-learning,2015,riverraid
A3C LSTM hs,6591.9,False,Asynchronous Methods for Deep Reinforcement Learning,2016,riverraid
Gorila,5310.3,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,riverraid
ES FF (1 hour) noop,5009.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,riverraid
DQN hs,4748.5,False,Deep Reinforcement Learning with Double Q-learning,2015,riverraid
UCT,4449,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,riverraid
MFEC,3868,False,Model-Free Episodic Control with State Aggregation,2020,riverraid
CGP,2914,False,Evolving simple programs for playing Atari games,2018,riverraid
SARSA,2650.0,False,,,riverraid
Best Learner,1904.3,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,riverraid
Agent57,19213.96,False,Agent57: Outperforming the Atari Human Benchmark,2020,gravitar
R2D2,15680.7,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,gravitar
MuZero (Res2 Adam),8006.93,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,gravitar
Go-Explore,7588,False,"First return, then explore",2020,gravitar
MuZero,6682.70,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,gravitar
GDI-H3(200M frames),5915,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,gravitar
GDI-I3,5905,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,gravitar
RND,3906,False,Exploration by Random Network Distillation,2018,gravitar
DreamerV2,3789,False,Mastering Atari with Discrete World Models,2020,gravitar
UCT,2850,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,gravitar
CGP,2350,False,Evolving simple programs for playing Atari games,2018,gravitar
NoisyNet-Dueling,2209,False,Noisy Networks for Exploration,2017,gravitar
A2C + SIL,1874.2,False,Self-Imitation Learning,2018,gravitar
Ape-X,1598.5,False,Distributed Prioritized Experience Replay,2018,gravitar
FQF,1406.0,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,gravitar
Intrinsic Reward Agent,1165.1,False,Large-Scale Study of Curiosity-Driven Learning,2018,gravitar
DQNMMCe,1078.3,False,Count-Based Exploration with the Successor Representation,2018,gravitar
QR-DQN-1,995,False,Distributional Reinforcement Learning with Quantile Regression,2017,gravitar
IQN,911,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,gravitar
ES FF (1 hour) noop,805.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,gravitar
Duel noop,588.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,gravitar
POP3D,557.17,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,gravitar
Prior noop,548.5,False,Prioritized Experience Replay,2015,gravitar
Gorila,538.4,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,gravitar
DQN-PixelCNN,498.3,False,Count-Based Exploration with Neural Density Models,2017,gravitar
DDQN+Pop-Art noop,483.5,False,Learning values across many orders of magnitude,2016,gravitar
DQN noop,473.0,False,Deep Reinforcement Learning with Double Q-learning,2015,gravitar
Persistent AL,446.92,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,gravitar
C51 noop,440.0,False,A Distributional Perspective on Reinforcement Learning,2017,gravitar
SARSA,429.0,False,,,gravitar
Advantage Learning,417.65,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,gravitar
DDQN (tuned) noop,412.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,gravitar
Best Learner,387.7,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,gravitar
IMPALA (deep),359.50,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,gravitar
A3C LSTM hs,320.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,gravitar
Nature DQN,306.7,False,,,gravitar
A3C FF hs,303.5,False,Asynchronous Methods for Deep Reinforcement Learning,2016,gravitar
DQN hs,298.0,False,Deep Reinforcement Learning with Double Q-learning,2015,gravitar
Duel hs,297.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,gravitar
Bootstrapped DQN,286.1,False,Deep Exploration via Bootstrapped DQN,2016,gravitar
Prior hs,269.5,False,Prioritized Experience Replay,2015,gravitar
A3C FF (1 day) hs,269.5,False,Asynchronous Methods for Deep Reinforcement Learning,2016,gravitar
A3C-CTS,238.68,False,Unifying Count-Based Exploration and Intrinsic Motivation,2016,gravitar
Prior+Duel noop,238.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,gravitar
DQN-CTS,238.0,False,Count-Based Exploration with Neural Density Models,2017,gravitar
DDQN (tuned) hs,200.5,False,Deep Reinforcement Learning with Double Q-learning,2015,gravitar
Prior+Duel hs,167.0,False,Deep Reinforcement Learning with Double Q-learning,2015,gravitar
MuZero,260.13,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,bowling
Go-Explore,260,False,"First return, then explore",2020,bowling
Agent57,251.18,False,Agent57: Outperforming the Atari Human Benchmark,2020,bowling
R2D2,219.5,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,bowling
GDI-H3(200M frames),205.2,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,bowling
GDI-I3,201.9,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,bowling
RUDDER,179,False,RUDDER: Return Decomposition for Delayed Rewards,2018,bowling
MuZero (Res2 Adam),131.65,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,bowling
FQF,102.3,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,bowling
DDQN+Pop-Art noop,102.1,False,Learning values across many orders of magnitude,2016,bowling
IQN,86.5,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,bowling
CGP,85.8,False,Evolving simple programs for playing Atari games,2018,bowling
C51 noop,81.8,False,A Distributional Perspective on Reinforcement Learning,2017,bowling
Reactor 500M,81.0,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,bowling
QR-DQN-1,77.2,False,Distributional Reinforcement Learning with Quantile Regression,2017,bowling
Persistent AL,71.59,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,bowling
DDQN (tuned) hs,69.6,False,Deep Reinforcement Learning with Double Q-learning,2015,bowling
DDQN (tuned) noop,68.1,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,bowling
Duel hs,65.7,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,bowling
Duel noop,65.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,bowling
Bootstrapped DQN,60.2,False,Deep Exploration via Bootstrapped DQN,2016,bowling
IMPALA (deep),59.92,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,bowling
Advantage Learning,57.41,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,bowling
DQN hs,56.5,False,Deep Reinforcement Learning with Double Q-learning,2015,bowling
Gorila,54,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,bowling
Prior hs,52,False,Prioritized Experience Replay,2015,bowling
DQN noop,50.4,False,Deep Reinforcement Learning with Double Q-learning,2015,bowling
Prior+Duel hs,50.4,False,Deep Reinforcement Learning with Double Q-learning,2015,bowling
DreamerV2,49,False,Mastering Atari with Discrete World Models,2020,bowling
Prior noop,47.9,False,Prioritized Experience Replay,2015,bowling
Prior+Duel noop,46.7,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,bowling
Best Learner,43.9,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,bowling
Nature DQN,42.4,False,,,bowling
A3C LSTM hs,41.8,False,Asynchronous Methods for Deep Reinforcement Learning,2016,bowling
POP3D,38.99,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,bowling
SARSA,36.4,False,,,bowling
A3C FF (1 day) hs,36.2,False,Asynchronous Methods for Deep Reinforcement Learning,2016,bowling
A3C FF hs,35.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,bowling
A2C + SIL,31.1,False,Self-Imitation Learning,2018,bowling
ES FF (1 hour) noop,30,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,bowling
Ape-X,17.6,False,Distributed Prioritized Experience Replay,2018,bowling
GDI-H3(200M frames),1000000,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,seaquest
Agent57,999997.63,False,Agent57: Outperforming the Atari Human Benchmark,2020,seaquest
R2D2,999996.7,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,seaquest
MuZero,999976.52,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,seaquest
MuZero (Res2 Adam),999659.18,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,seaquest
GDI-I3,943910,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,seaquest
Ape-X,392952.3,False,Distributed Prioritized Experience Replay,2018,seaquest
C51 noop,266434.0,False,A Distributional Perspective on Reinforcement Learning,2017,seaquest
Duel noop,50254.2,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,seaquest
Duel hs,37361.6,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,seaquest
IQN,30140,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,seaquest
Prior noop,26357.8,False,Prioritized Experience Replay,2015,seaquest
Prior hs,25463.7,False,Prioritized Experience Replay,2015,seaquest
NoisyNet-Dueling,16754,False,Noisy Networks for Exploration,2017,seaquest
DDQN (tuned) noop,16452.7,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,seaquest
DDQN (tuned) hs,14498.0,False,Deep Reinforcement Learning with Double Q-learning,2015,seaquest
Persistent AL,13230.74,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,seaquest
DDQN+Pop-Art noop,10932.3,False,Learning values across many orders of magnitude,2016,seaquest
Gorila,10145.9,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,seaquest
Bootstrapped DQN,9083.1,False,Deep Exploration via Bootstrapped DQN,2016,seaquest
Advantage Learning,8670.5,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,seaquest
QR-DQN-1,8268,False,Distributional Reinforcement Learning with Quantile Regression,2017,seaquest
DreamerV2,7480,False,Mastering Atari with Discrete World Models,2020,seaquest
Recurrent Rational DQN Average,7460,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,seaquest
DARQN soft,7263,False,Deep Attention Recurrent Q-Network,2015,seaquest
Rational DQN Average,6603,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,seaquest
DQN noop,5860.6,False,Deep Reinforcement Learning with Double Q-learning,2015,seaquest
VPN,5628,False,Value Prediction Network,2017,seaquest
Nature DQN,5286.0,False,,,seaquest
UCT,5132.4,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,seaquest
DQN hs,4216.7,False,Deep Reinforcement Learning with Double Q-learning,2015,seaquest
A2C + SIL,2456.5,False,Self-Imitation Learning,2018,seaquest
A3C FF hs,2355.4,False,Asynchronous Methods for Deep Reinforcement Learning,2016,seaquest
A3C FF (1 day) hs,2300.2,False,Asynchronous Methods for Deep Reinforcement Learning,2016,seaquest
DDRL A3C,1832,False,Distributed Deep Reinforcement Learning: Learn how to play Atari games in 21 minutes,2018,seaquest
POP3D,1807.47,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,seaquest
IMPALA (deep),1753.20,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,seaquest
DQN Best,1740,False,Playing Atari with Deep Reinforcement Learning,2013,seaquest
MAC,1703.4,False,Mean Actor Critic,2017,seaquest
Prior+Duel hs,1431.2,False,Deep Reinforcement Learning with Double Q-learning,2015,seaquest
ES FF (1 hour) noop,1390.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,seaquest
A3C LSTM hs,1326.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,seaquest
Prior+Duel noop,931.6,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,seaquest
CGP,724,False,Evolving simple programs for playing Atari games,2018,seaquest
SARSA,675.5,False,,,seaquest
Best Learner,664.8,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,seaquest
Discrete Latent Space World Model (VQ-VAE),635,False,Smaller World Models for Reinforcement Learning,2020,seaquest
Rainbow+SEER,561.2,False,Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings,2021,seaquest
CURL,408,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,seaquest
IDVQ + DRSC + XNES,320,False,Playing Atari with Six Neurons,2018,seaquest
SAC,211.6,False,Soft Actor-Critic for Discrete Action Settings,2019,seaquest
DT,2.4,False,Decision Transformer: Reinforcement Learning via Sequence Modeling,2021,seaquest
MuZero,741812.63,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,alien
Agent57,297638.17,False,Agent57: Outperforming the Atari Human Benchmark,2020,alien
R2D2,229496.9,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,alien
MuZero (Res2 Adam),70192.35,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,alien
GDI-H3(200M frames),48735,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,alien
GDI-I3,43384,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,alien
Ape-X,40804.9,False,Distributed Prioritized Experience Replay,2018,alien
FQF,16754.6,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,alien
IMPALA (deep),15962.10,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,alien
Reactor 500M,12689.1,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,alien
UCT,7785,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,alien
IQN,7022,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,alien
NoisyNet-Dueling,5778,False,Noisy Networks for Exploration,2017,alien
Persistent AL,5699.81,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,alien
Advantage Learning,4990.91,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,alien
QR-DQN-1,4871,False,Distributional Reinforcement Learning with Quantile Regression,2017,alien
Duel noop,4461.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,alien
Prior noop,4203.8,False,Prioritized Experience Replay,2015,alien
DreamerV2,3967,False,Mastering Atari with Discrete World Models,2020,alien
Prior+Duel noop,3941.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,alien
DDQN (tuned) noop,3747.7,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,alien
DDQN+Pop-Art noop,3213.5,False,Learning values across many orders of magnitude,2016,alien
C51 noop,3166.0,False,A Distributional Perspective on Reinforcement Learning,2017,alien
Nature DQN,3069.0,False,,,alien
Bootstrapped DQN,2436.6,False,Deep Exploration via Bootstrapped DQN,2016,alien
A2C + SIL,2242.2,False,Self-Imitation Learning,2018,alien
CGP,1978,False,Evolving simple programs for playing Atari games,2018,alien
DQN noop,1620.0,False,Deep Reinforcement Learning with Double Q-learning,2015,alien
POP3D,1510.8,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,alien
Duel hs,1486.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,alien
VPN,1429,False,Value Prediction Network,2017,alien
Prior hs,1334.7,False,Prioritized Experience Replay,2015,alien
Rainbow+SEER,1172.6,False,Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings,2021,alien
CURL,1148.2,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,alien
DDQN (tuned) hs,1033.4,False,Deep Reinforcement Learning with Double Q-learning,2015,alien
ES FF (1 hour) noop,994.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,alien
A3C LSTM hs,945.3,False,Asynchronous Methods for Deep Reinforcement Learning,2016,alien
Best Learner,939.2,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,alien
Prior+Duel hs,823.7,False,Deep Reinforcement Learning with Double Q-learning,2015,alien
Prior+Duel hs,823.7,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,alien
Gorila,813.5,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,alien
DQN hs,634.0,False,Deep Reinforcement Learning with Double Q-learning,2015,alien
A3C FF hs,518.4,False,Asynchronous Methods for Deep Reinforcement Learning,2016,alien
SAC,216.9,False,Soft Actor-Critic for Discrete Action Settings,2019,alien
A3C FF (1 day) hs,182.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,alien
SARSA,103.2,False,,,alien
GDI-H3(200M frames),787985,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,demonattack
GDI-I3,675530,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,demonattack
RIMs-PPO,230324,False,,,demonattack
MuZero,143964.26,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,demonattack
MuZero (Res2 Adam),143838.04,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,demonattack
Agent57,143161.44,False,Agent57: Outperforming the Atari Human Benchmark,2020,demonattack
R2D2,140002.3,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,demonattack
Ape-X,133086.4,False,Distributed Prioritized Experience Replay,2018,demonattack
IMPALA (deep),132826.98,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,demonattack
C51 noop,130955.0,False,A Distributional Perspective on Reinforcement Learning,2017,demonattack
IQN,128580,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,demonattack
QR-DQN-1,121551,False,Distributional Reinforcement Learning with Quantile Regression,2017,demonattack
A3C LSTM hs,115201.9,False,Asynchronous Methods for Deep Reinforcement Learning,2016,demonattack
Reactor 500M,115154.0,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,demonattack
A3C FF hs,113308.4,False,Asynchronous Methods for Deep Reinforcement Learning,2016,demonattack
A3C FF (1 day) hs,84997.5,False,Asynchronous Methods for Deep Reinforcement Learning,2016,demonattack
Bootstrapped DQN,82610,False,Deep Exploration via Bootstrapped DQN,2016,demonattack
DreamerV2,82263,False,Mastering Atari with Discrete World Models,2020,demonattack
Prior+Duel hs,73371.3,False,Deep Reinforcement Learning with Double Q-learning,2015,demonattack
Prior+Duel noop,72878.6,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,demonattack
Prior noop,71846.4,False,Prioritized Experience Replay,2015,demonattack
Persistent AL,70908.17,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,demonattack
DDQN (tuned) hs,69803.4,False,Deep Reinforcement Learning with Double Q-learning,2015,demonattack
NoisyNet-Dueling,69311,False,Noisy Networks for Exploration,2017,demonattack
DDQN+Pop-Art noop,63644.9,False,Learning values across many orders of magnitude,2016,demonattack
Prior hs,61277.5,False,Prioritized Experience Replay,2015,demonattack
POP3D,61147.33,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,demonattack
Duel noop,60813.3,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,demonattack
DDQN (tuned) noop,58044.2,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,demonattack
Duel hs,56322.8,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,demonattack
UCT,28158.8,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,demonattack
Advantage Learning,27153.48,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,demonattack
Gorila,14880.1,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,demonattack
DQN hs,12550.7,False,Deep Reinforcement Learning with Double Q-learning,2015,demonattack
DQN noop,12149.4,False,Deep Reinforcement Learning with Double Q-learning,2015,demonattack
A2C + SIL,10140.5,False,Self-Imitation Learning,2018,demonattack
Nature DQN,9711.0,False,,,demonattack
CGP,2387,False,Evolving simple programs for playing Atari games,2018,demonattack
ES FF (1 hour) noop,1166.5,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,demonattack
CURL,834,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,demonattack
Best Learner,520.5,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,demonattack
IDVQ + DRSC + XNES,325,False,Playing Atari with Six Neurons,2018,demonattack
SARSA,0.0,False,,,demonattack
MuZero,725853.90,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,zaxxon
Agent57,249808.9,False,Agent57: Outperforming the Atari Human Benchmark,2020,zaxxon
R2D2,224910.7,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,zaxxon
GDI-H3(200M frames),216020,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,zaxxon
MuZero (Res2 Adam),154131.86,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,zaxxon
GDI-I3,109140,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,zaxxon
DreamerV2,50699,False,Mastering Atari with Discrete World Models,2020,zaxxon
Ape-X,42285.5,False,Distributed Prioritized Experience Replay,2018,zaxxon
IMPALA (deep),32935.50,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,zaxxon
A3C FF hs,24622.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,zaxxon
A3C LSTM hs,23519.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,zaxxon
UCT,22610,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,zaxxon
IQN,21772,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,zaxxon
RIMs-PPO,15000,False,,,zaxxon
NoisyNet-Dueling,14874,False,Noisy Networks for Exploration,2017,zaxxon
DDQN+Pop-Art noop,14402.0,False,Learning values across many orders of magnitude,2016,zaxxon
Prior+Duel noop,13886.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,zaxxon
QR-DQN-1,13112,False,Distributional Reinforcement Learning with Quantile Regression,2017,zaxxon
Duel noop,12944.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,zaxxon
Bootstrapped DQN,11491.7,False,Deep Exploration via Bootstrapped DQN,2016,zaxxon
Prior+Duel hs,11320.0,False,Deep Reinforcement Learning with Double Q-learning,2015,zaxxon
C51 noop,10513.0,False,A Distributional Perspective on Reinforcement Learning,2017,zaxxon
Prior noop,10469.0,False,Prioritized Experience Replay,2015,zaxxon
Duel hs,10164.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,zaxxon
DDQN (tuned) noop,10163.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,zaxxon
Prior hs,9474.0,False,Prioritized Experience Replay,2015,zaxxon
POP3D,9472,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,zaxxon
A2C + SIL,9164.2,False,Self-Imitation Learning,2018,zaxxon
Advantage Learning,9129.61,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,zaxxon
DDQN (tuned) hs,8593.0,False,Deep Reinforcement Learning with Double Q-learning,2015,zaxxon
ES FF (1 hour) noop,6380.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,zaxxon
Gorila,6159.4,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,zaxxon
DQN noop,5363.0,False,Deep Reinforcement Learning with Double Q-learning,2015,zaxxon
Nature DQN,4977.0,False,,,zaxxon
DQN hs,4412.0,False,Deep Reinforcement Learning with Double Q-learning,2015,zaxxon
Best Learner,3365.1,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,zaxxon
CGP,2980,False,Evolving simple programs for playing Atari games,2018,zaxxon
A3C FF (1 day) hs,2659.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,zaxxon
SARSA,21.4,False,,,zaxxon
GDI-H3(200M frames),999999,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,asterix
R2D2,999153.3,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,asterix
MuZero,998425.00,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,asterix
Agent57,991384.42,False,Agent57: Outperforming the Atari Human Benchmark,2020,asterix
MuZero (Res2 Adam),862406.65,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,asterix
GDI-I3,759910,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,asterix
FQF,578388.5,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,asterix
C51 noop,406211,False,A Distributional Perspective on Reinforcement Learning,2017,asterix
Prior+Duel noop,375080,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,asterix
Prior+Duel hs,364200,False,Deep Reinforcement Learning with Double Q-learning,2015,asterix
Prior+Duel hs,364200.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,asterix
IQN,342016,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,asterix
Ape-X,313305,False,Distributed Prioritized Experience Replay,2018,asterix
IMPALA (deep),300732.00,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,asterix
UCT,290700,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,asterix
QR-DQN-1,261025,False,Distributional Reinforcement Learning with Quantile Regression,2017,asterix
Reactor 500M,205914.0,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,asterix
DreamerV2,72311,False,Mastering Atari with Discrete World Models,2020,asterix
Prior noop,31527,False,Prioritized Experience Replay,2015,asterix
NoisyNet-Dueling,28350,False,Noisy Networks for Exploration,2017,asterix
Duel noop,28188,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,asterix
Prior hs,22484.5,False,Prioritized Experience Replay,2015,asterix
A3C FF hs,22140.5,False,Asynchronous Methods for Deep Reinforcement Learning,2016,asterix
RIMs-PPO,21040,False,,,asterix
Bootstrapped DQN,19713.2,False,Deep Exploration via Bootstrapped DQN,2016,asterix
Persistent AL,19564.9,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,asterix
DDQN+Pop-Art noop,18919.5,False,Learning values across many orders of magnitude,2016,asterix
Rational DQN Average,18109,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,asterix
A2C + SIL,17984.2,False,Self-Imitation Learning,2018,asterix
DDQN (tuned) noop,17356.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,asterix
A3C LSTM hs,17244.5,False,Asynchronous Methods for Deep Reinforcement Learning,2016,asterix
DDQN (tuned) hs,16837,False,Deep Reinforcement Learning with Double Q-learning,2015,asterix
Duel hs,15840,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,asterix
Advantage Learning,12852.08,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,asterix
Recurrent Rational DQN Average,12621,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,asterix
A3C FF (1 day) hs,6723,False,Asynchronous Methods for Deep Reinforcement Learning,2016,asterix
Nature DQN,6012,False,,,asterix
DQN noop,4359,False,Deep Reinforcement Learning with Double Q-learning,2015,asterix
POP3D,4310.67,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,asterix
Gorila,3324.7,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,asterix
DQN hs,3170.5,False,Deep Reinforcement Learning with Double Q-learning,2015,asterix
CGP,1880,False,Evolving simple programs for playing Atari games,2018,asterix
ES FF (1 hour) noop,1440,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,asterix
SARSA,1332,False,,,asterix
Best Learner,987.3,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,asterix
CURL,524.3,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,asterix
SAC,272,False,Soft Actor-Critic for Discrete Action Settings,2019,asterix
Agent57,998532.37,False,Agent57: Outperforming the Atari Human Benchmark,2020,yarsrevenge
R2D2,995048.4,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,yarsrevenge
GDI-I3,972000,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,yarsrevenge
GDI-H3(200M frames),968090,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,yarsrevenge
MuZero,553311.46,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,yarsrevenge
MuZero (Res2 Adam),219838.09,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,yarsrevenge
DreamerV2,156748,False,Mastering Atari with Discrete World Models,2020,yarsrevenge
Ape-X,148594.8,False,Distributed Prioritized Experience Replay,2018,yarsrevenge
NoisyNet-Dueling,86101,False,Noisy Networks for Exploration,2017,yarsrevenge
IMPALA (deep),84231.14,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,yarsrevenge
RUDDER,60577,False,RUDDER: Return Decomposition for Delayed Rewards,2018,yarsrevenge
CGP,28838.2,False,Evolving simple programs for playing Atari games,2018,yarsrevenge
IQN,28379,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,yarsrevenge
QR-DQN-1,26447,False,Distributional Reinforcement Learning with Quantile Regression,2017,yarsrevenge
Advantage Learning,24240.03,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,yarsrevenge
TRPO-hash,34.0,False,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning,2016,freeway
IQN,34,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,freeway
NoisyNet-Dueling,34,False,Noisy Networks for Exploration,2017,freeway
QR-DQN-1,34,False,Distributional Reinforcement Learning with Quantile Regression,2017,freeway
Go-Explore,34,False,"First return, then explore",2020,freeway
GDI-H3,34,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,freeway
GDI-H3(200M frames),34,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,freeway
C51 noop,33.9,False,A Distributional Perspective on Reinforcement Learning,2017,freeway
Bootstrapped DQN,33.9,False,Deep Exploration via Bootstrapped DQN,2016,freeway
MuZero (Res2 Adam),33.87,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,freeway
Prior noop,33.7,False,Prioritized Experience Replay,2015,freeway
Ape-X,33.7,False,Distributed Prioritized Experience Replay,2018,freeway
DDQN+Pop-Art noop,33.4,False,Learning values across many orders of magnitude,2016,freeway
DDQN (tuned) noop,33.3,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,freeway
MuZero,33.03,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,freeway
Prior+Duel noop,33.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,freeway
DQN-CTS,33.0,False,Count-Based Exploration with Neural Density Models,2017,freeway
DreamerV2,33,False,Mastering Atari with Discrete World Models,2020,freeway
Intrinsic Reward Agent,32.8,False,Large-Scale Study of Curiosity-Driven Learning,2018,freeway
Agent57,32.59,False,Agent57: Outperforming the Atari Human Benchmark,2020,freeway
R2D2,32.5,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,freeway
Persistent AL,32.3,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,freeway
A2C + SIL,32.2,False,Self-Imitation Learning,2018,freeway
Advantage Learning,31.72,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,freeway
DQN-PixelCNN,31.7,False,Count-Based Exploration with Neural Density Models,2017,freeway
ES FF (1 hour) noop,31.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,freeway
DQN noop,30.8,False,Deep Reinforcement Learning with Double Q-learning,2015,freeway
A3C-CTS,30.48,False,Unifying Count-Based Exploration and Intrinsic Motivation,2016,freeway
Nature DQN,30.3,False,,,freeway
Sarsa-ε,29.9,False,Count-Based Exploration in Feature Space for Reinforcement Learning,2017,freeway
DQNMMCe,29.5,False,Count-Based Exploration with the Successor Representation,2018,freeway
Discrete Latent Space World Model (VQ-VAE),29,False,Smaller World Models for Reinforcement Learning,2020,freeway
Prior hs,28.9,False,Prioritized Experience Replay,2015,freeway
DDQN (tuned) hs,28.8,False,Deep Reinforcement Learning with Double Q-learning,2015,freeway
Prior+Duel hs,28.2,False,Deep Reinforcement Learning with Double Q-learning,2015,freeway
CGP,28.2,False,Evolving simple programs for playing Atari games,2018,freeway
CURL,27.9,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,freeway
MP-EB,27.0,False,Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models,2015,freeway
DQN hs,26.9,False,Deep Reinforcement Learning with Double Q-learning,2015,freeway
Best Baseline,22.5,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,freeway
ENAS,22,False,Optimizing the Neural Architecture of Reinforcement Learning Agents,2020,freeway
SPOS,22,False,Optimizing the Neural Architecture of Reinforcement Learning Agents,2020,freeway
POP3D,21.21,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,freeway
SARSA,19.7,False,,,freeway
Best Learner,19.1,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,freeway
Gorila,10.2,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,freeway
SAC,4.4,False,Soft Actor-Critic for Discrete Action Settings,2019,freeway
UCT,0.4,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,freeway
Duel hs,0.2,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,freeway
A3C FF (1 day) hs,0.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,freeway
A3C FF hs,0.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,freeway
A3C LSTM hs,0.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,freeway
Duel noop,0.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,freeway
Sarsa-φ-EB,0.0,False,Count-Based Exploration in Feature Space for Reinforcement Learning,2017,freeway
IMPALA (deep),0.00,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,freeway
Agent57,29660.08,False,Agent57: Outperforming the Atari Human Benchmark,2020,amidar
R2D2,29321.4,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,amidar
MuZero,28634.39,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,amidar
Ape-X,8659.2,False,Distributed Prioritized Experience Replay,2018,amidar
NoisyNet-Dueling,3537,False,Noisy Networks for Exploration,2017,amidar
FQF,3165.3,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,amidar
IQN,2946,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,amidar
DreamerV2,2577,False,Mastering Atari with Discrete World Models,2020,amidar
Duel noop,2354.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,amidar
Prior+Duel noop,2296.8,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,amidar
Prior noop,1838.9,False,Prioritized Experience Replay,2015,amidar
DDQN (tuned) noop,1793.3,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,amidar
C51 noop,1735.0,False,A Distributional Perspective on Reinforcement Learning,2017,amidar
QR-DQN-1,1641,False,Distributional Reinforcement Learning with Quantile Regression,2017,amidar
Advantage Learning,1557.43,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,amidar
IMPALA (deep),1554.79,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,amidar
Persistent AL,1451.65,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,amidar
GDI-I3,1442,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,amidar
A2C + SIL,1362,False,Self-Imitation Learning,2018,amidar
Bootstrapped DQN,1272.5,False,Deep Exploration via Bootstrapped DQN,2016,amidar
MuZero (Res2 Adam),1197.38,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,amidar
GDI-H3(200M frames),1065,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,amidar
Reactor 500M,1015.8,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,amidar
DQN noop,978.0,False,Deep Reinforcement Learning with Double Q-learning,2015,amidar
DDQN+Pop-Art noop,782.5,False,Learning values across many orders of magnitude,2016,amidar
Nature DQN,739.5,False,,,amidar
POP3D,729.15,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,amidar
VPN,641,False,Value Prediction Network,2017,amidar
A3C FF (1 day) hs,283.9,False,Asynchronous Methods for Deep Reinforcement Learning,2016,amidar
A3C FF hs,263.9,False,Asynchronous Methods for Deep Reinforcement Learning,2016,amidar
Rainbow+SEER,250.5,False,Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings,2021,amidar
Prior+Duel hs,238.4,False,Deep Reinforcement Learning with Double Q-learning,2015,amidar
Prior+Duel hs,238.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,amidar
CURL,232.3,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,amidar
CGP,199,False,Evolving simple programs for playing Atari games,2018,amidar
Gorila,189.2,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,amidar
SARSA,183.6,False,,,amidar
UCT,180.3,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,amidar
DQN hs,178.4,False,Deep Reinforcement Learning with Double Q-learning,2015,amidar
A3C LSTM hs,173.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,amidar
Duel hs,172.7,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,amidar
DDQN (tuned) hs,169.1,False,Deep Reinforcement Learning with Double Q-learning,2015,amidar
Prior hs,129.1,False,Prioritized Experience Replay,2015,amidar
ES FF (1 hour) noop,112.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,amidar
Best Learner,103.4,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,amidar
SAC,7.9,False,Soft Actor-Critic for Discrete Action Settings,2019,amidar
GDI-I3,24,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,tennis
GDI-H3(200M frames),24,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,tennis
Ape-X,23.9,False,Distributed Prioritized Experience Replay,2018,tennis
Agent57,23.84,False,Agent57: Outperforming the Atari Human Benchmark,2020,tennis
IQN,23.6,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,tennis
QR-DQN-1,23.6,False,Distributional Reinforcement Learning with Quantile Regression,2017,tennis
C51 noop,23.1,False,A Distributional Perspective on Reinforcement Learning,2017,tennis
Recurrent Rational DQN Average,20.6,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,tennis
Rational DQN Average,20.5,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,tennis
DreamerV2,14,False,Mastering Atari with Discrete World Models,2020,tennis
DQN noop,12.2,False,Deep Reinforcement Learning with Double Q-learning,2015,tennis
DDQN+Pop-Art noop,12.1,False,Learning values across many orders of magnitude,2016,tennis
DQN hs,11.1,False,Deep Reinforcement Learning with Double Q-learning,2015,tennis
Duel noop,5.1,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,tennis
Duel hs,4.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,tennis
UCT,2.8,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,tennis
IMPALA (deep),0.55,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,tennis
SARSA,0.0,False,,,tennis
Prior noop,0.0,False,Prioritized Experience Replay,2015,tennis
Prior+Duel noop,0.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,tennis
Bootstrapped DQN,0,False,Deep Exploration via Bootstrapped DQN,2016,tennis
MuZero,0.00,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,tennis
CGP,0,False,Evolving simple programs for playing Atari games,2018,tennis
NoisyNet-Dueling,0,False,Noisy Networks for Exploration,2017,tennis
Advantage Learning,0,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,tennis
MuZero (Res2 Adam),0,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,tennis
Nature DQN,-2.5,False,,,tennis
Gorila,-0.7,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,tennis
DDQN (tuned) noop,-22.8,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,tennis
DDQN (tuned) hs,-7.8,False,Deep Reinforcement Learning with Double Q-learning,2015,tennis
Prior+Duel hs,-13.2,False,Deep Reinforcement Learning with Double Q-learning,2015,tennis
Prior hs,-5.3,False,Prioritized Experience Replay,2015,tennis
A3C FF hs,-6.3,False,Asynchronous Methods for Deep Reinforcement Learning,2016,tennis
A3C LSTM hs,-6.4,False,Asynchronous Methods for Deep Reinforcement Learning,2016,tennis
A3C FF (1 day) hs,-10.2,False,Asynchronous Methods for Deep Reinforcement Learning,2016,tennis
ES FF (1 hour) noop,-4.5,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,tennis
R2D2,-0.1,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,tennis
Best Learner,-0.1,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,tennis
POP3D,-8.32,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,tennis
A2C + SIL,-17.3,False,Self-Imitation Learning,2018,tennis
Agent57,44199.93,False,Agent57: Outperforming the Atari Human Benchmark,2020,solaris
Go-Explore,19671,False,"First return, then explore",2020,solaris
GDI-I3,11074,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,solaris
GDI-H3(200M frames),9105,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,solaris
CGP,8324,False,Evolving simple programs for playing Atari games,2018,solaris
IQN,8007,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,solaris
QR-DQN-1,6740,False,Distributional Reinforcement Learning with Quantile Regression,2017,solaris
NoisyNet-Dueling,6522,False,Noisy Networks for Exploration,2017,solaris
MuZero (Res2 Adam),5132.95,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,solaris
Advantage Learning,4785.16,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,solaris
R2D2,3787.2,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,solaris
RND,3282,False,Exploration by Random Network Distillation,2018,solaris
Ape-X,2892.9,False,Distributed Prioritized Experience Replay,2018,solaris
IMPALA (deep),2365.00,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,solaris
DQNMMCe,2244.6,False,Count-Based Exploration with the Successor Representation,2018,solaris
DreamerV2,922,False,Mastering Atari with Discrete World Models,2020,solaris
MuZero,56.62,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,solaris
MuZero (Res2 Adam),27219.8,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,bankheist
R2D2,24235.9,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,bankheist
Agent57,23071.5,False,Agent57: Outperforming the Atari Human Benchmark,2020,bankheist
Ape-X,1716.4,False,Distributed Prioritized Experience Replay,2018,bankheist
Duel noop,1611.9,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,bankheist
Prior+Duel noop,1503.1,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,bankheist
IQN,1416,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,bankheist
GDI-I3,1401,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,bankheist
GDI-H3(200M frames),1380,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,bankheist
NoisyNet-Dueling,1318,False,Noisy Networks for Exploration,2017,bankheist
MuZero,1278.98,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,bankheist
Reactor 500M,1259.7,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,bankheist
QR-DQN-1,1249,False,Distributional Reinforcement Learning with Quantile Regression,2017,bankheist
IMPALA (deep),1223.15,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,bankheist
POP3D,1212.23,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,bankheist
Bootstrapped DQN,1208,False,Deep Exploration via Bootstrapped DQN,2016,bankheist
A2C + SIL,1137.8,False,Self-Imitation Learning,2018,bankheist
Duel hs,1129.3,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,bankheist
DreamerV2,1126,False,Mastering Atari with Discrete World Models,2020,bankheist
DDQN+Pop-Art noop,1103.3,False,Learning values across many orders of magnitude,2016,bankheist
Prior noop,1054.6,False,Prioritized Experience Replay,2015,bankheist
DDQN (tuned) noop,1030.6,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,bankheist
Prior+Duel hs,1004.6,False,Deep Reinforcement Learning with Double Q-learning,2015,bankheist
C51 noop,976.0,False,A Distributional Perspective on Reinforcement Learning,2017,bankheist
A3C FF hs,970.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,bankheist
A3C FF (1 day) hs,946.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,bankheist
A3C LSTM hs,932.8,False,Asynchronous Methods for Deep Reinforcement Learning,2016,bankheist
DDQN (tuned) hs,886.0,False,Deep Reinforcement Learning with Double Q-learning,2015,bankheist
Prior hs,876.6,False,Prioritized Experience Replay,2015,bankheist
Persistent AL,874.99,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,bankheist
Advantage Learning,633.63,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,bankheist
UCT,497.8,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,bankheist
DQN noop,455.0,False,Deep Reinforcement Learning with Double Q-learning,2015,bankheist
Nature DQN,429.7,False,,,bankheist
Gorila,399.4,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,bankheist
DQN hs,312.7,False,Deep Reinforcement Learning with Double Q-learning,2015,bankheist
Rainbow+SEER,276.6,False,Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings,2021,bankheist
ES FF (1 hour) noop,225.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,bankheist
CURL,193.7,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,bankheist
Best Learner,190.8,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,bankheist
CGP,148,False,Evolving simple programs for playing Atari games,2018,bankheist
Discrete Latent Space World Model (VQ-VAE),121.6,False,Smaller World Models for Reinforcement Learning,2020,bankheist
SARSA,67.4,False,,,bankheist
GDI-I3,488830,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,gopher
GDI-H3(200M frames),473560,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,gopher
MuZero,130345.58,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,gopher
R2D2,124776.3,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,gopher
MuZero (Res2 Adam),122882.5,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,gopher
Ape-X,120500.9,False,Distributed Prioritized Experience Replay,2018,gopher
IQN,118365,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,gopher
Agent57,117777.08,False,Agent57: Outperforming the Atari Human Benchmark,2020,gopher
QR-DQN-1,113585,False,Distributional Reinforcement Learning with Quantile Regression,2017,gopher
Prior+Duel hs,105148.4,False,Deep Reinforcement Learning with Double Q-learning,2015,gopher
Prior+Duel noop,104368.2,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,gopher
DreamerV2,92282,False,Mastering Atari with Discrete World Models,2020,gopher
IMPALA (deep),66782.30,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,gopher
DDQN+Pop-Art noop,56218.2,False,Learning values across many orders of magnitude,2016,gopher
NoisyNet-Dueling,38909,False,Noisy Networks for Exploration,2017,gopher
Prior hs,34858.8,False,Prioritized Experience Replay,2015,gopher
C51 noop,33641.0,False,A Distributional Perspective on Reinforcement Learning,2017,gopher
Prior noop,32487.2,False,Prioritized Experience Replay,2015,gopher
A2C + SIL,23304.2,False,Self-Imitation Learning,2018,gopher
UCT,20560,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,gopher
Duel hs,20051.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,gopher
Bootstrapped DQN,17438.4,False,Deep Exploration via Bootstrapped DQN,2016,gopher
A3C LSTM hs,17106.8,False,Asynchronous Methods for Deep Reinforcement Learning,2016,gopher
Duel noop,15718.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,gopher
DDQN (tuned) hs,15253.0,False,Deep Reinforcement Learning with Double Q-learning,2015,gopher
DDQN (tuned) noop,14840.8,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,gopher
Advantage Learning,11912.68,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,gopher
Persistent AL,10611.81,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,gopher
A3C FF hs,10022.8,False,Asynchronous Methods for Deep Reinforcement Learning,2016,gopher
DQN noop,8777.4,False,Deep Reinforcement Learning with Double Q-learning,2015,gopher
Nature DQN,8520.0,False,,,gopher
A3C FF (1 day) hs,8442.8,False,Asynchronous Methods for Deep Reinforcement Learning,2016,gopher
DQN hs,8190.4,False,Deep Reinforcement Learning with Double Q-learning,2015,gopher
POP3D,6207,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,gopher
DARQN soft,5356,False,Deep Attention Recurrent Q-Network,2015,gopher
Gorila,4373.0,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,gopher
SARSA,2368.0,False,,,gopher
CGP,1696,False,Evolving simple programs for playing Atari games,2018,gopher
Best Learner,1288.3,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,gopher
CURL,801.4,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,gopher
ES FF (1 hour) noop,582.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,gopher
CGP,993010,False,Evolving simple programs for playing Atari games,2018,defender
GDI-H3(200M frames),970540,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,defender
GDI-I3,893110,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,defender
MuZero,839642.95,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,defender
Agent57,677642.78,False,Agent57: Outperforming the Atari Human Benchmark,2020,defender
R2D2,665792.0,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,defender
MuZero (Res2 Adam),557200.75,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,defender
Ape-X,411943.5,False,Distributed Prioritized Experience Replay,2018,defender
Reactor 500M,223025.0,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,defender
IMPALA (deep),185203.00,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,defender
IQN,53537,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,defender
QR-DQN-1,47887,False,Distributional Reinforcement Learning with Quantile Regression,2017,defender
NoisyNet-Dueling,42253,False,Noisy Networks for Exploration,2017,defender
Duel noop,42214.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,defender
Prior+Duel noop,41324.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,defender
Prior+Duel hs,34415.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,defender
Persistent AL,32038.93,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,defender
Advantage Learning,30643.59,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,defender
Agent57,2354.91,False,Agent57: Outperforming the Atari Human Benchmark,2020,tutankham
MuZero,491.48,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,tutankham
GDI-I3,423.9,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,tutankham
GDI-H3(200M frames),418.2,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,tutankham
R2D2,395.3,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,tutankham
MuZero (Res2 Adam),347.99,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,tutankham
A2C + SIL,340.5,False,Self-Imitation Learning,2018,tutankham
QR-DQN-1,297,False,Distributional Reinforcement Learning with Quantile Regression,2017,tutankham
IQN,293,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,tutankham
IMPALA (deep),292.11,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,tutankham
C51 noop,280.0,False,A Distributional Perspective on Reinforcement Learning,2017,tutankham
Ape-X,272.6,False,Distributed Prioritized Experience Replay,2018,tutankham
NoisyNet-Dueling,269,False,Noisy Networks for Exploration,2017,tutankham
DreamerV2,264,False,Mastering Atari with Discrete World Models,2020,tutankham
Prior+Duel noop,245.9,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,tutankham
Advantage Learning,245.22,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,tutankham
POP3D,241.21,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,tutankham
UCT,225.5,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,tutankham
DDQN (tuned) noop,218.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,tutankham
Bootstrapped DQN,214.8,False,Deep Exploration via Bootstrapped DQN,2016,tutankham
Duel noop,211.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,tutankham
Prior noop,204.6,False,Prioritized Experience Replay,2015,tutankham
DARQN soft,197,False,Deep Attention Recurrent Q-Network,2015,tutankham
Nature DQN,186.7,False,,,tutankham
Recurrent Rational DQN Average,184,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,tutankham
DDQN+Pop-Art noop,183.9,False,Learning values across many orders of magnitude,2016,tutankham
Rational DQN Average,179,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,tutankham
A3C FF hs,156.3,False,Asynchronous Methods for Deep Reinforcement Learning,2016,tutankham
A3C LSTM hs,144.2,False,Asynchronous Methods for Deep Reinforcement Learning,2016,tutankham
ES FF (1 hour) noop,130.3,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,tutankham
Gorila,118.5,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,tutankham
Best Learner,114.3,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,tutankham
Prior+Duel hs,108.6,False,Deep Reinforcement Learning with Double Q-learning,2015,tutankham
SARSA,98.2,False,,,tutankham
DDQN (tuned) hs,92.2,False,Deep Reinforcement Learning with Double Q-learning,2015,tutankham
DQN noop,68.1,False,Deep Reinforcement Learning with Double Q-learning,2015,tutankham
Prior hs,56.9,False,Prioritized Experience Replay,2015,tutankham
Duel hs,48.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,tutankham
DQN hs,45.6,False,Deep Reinforcement Learning with Double Q-learning,2015,tutankham
A3C FF (1 day) hs,26.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,tutankham
CGP,0,False,Evolving simple programs for playing Atari games,2018,tutankham
MuZero,197126.00,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,wizardofwor
Agent57,157306.41,False,Agent57: Outperforming the Atari Human Benchmark,2020,wizardofwor
R2D2,144362.7,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,wizardofwor
UCT,105500,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,wizardofwor
MuZero (Res2 Adam),100096.6,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,wizardofwor
GDI-I3,64293,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,wizardofwor
GDI-H3(200M frames),63735,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,wizardofwor
Ape-X,46204,False,Distributed Prioritized Experience Replay,2018,wizardofwor
FQF,44782.6,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,wizardofwor
IQN,31190,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,wizardofwor
QR-DQN-1,25061,False,Distributional Reinforcement Learning with Quantile Regression,2017,wizardofwor
A3C LSTM hs,18082.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,wizardofwor
A3C FF hs,17244.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,wizardofwor
DreamerV2,12851,False,Mastering Atari with Discrete World Models,2020,wizardofwor
Prior+Duel noop,12352.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,wizardofwor
Prior+Duel hs,10471.0,False,Deep Reinforcement Learning with Double Q-learning,2015,wizardofwor
Gorila,10431.0,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,wizardofwor
Advantage Learning,9541.14,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,wizardofwor
C51 noop,9300.0,False,A Distributional Perspective on Reinforcement Learning,2017,wizardofwor
IMPALA (deep),9157.50,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,wizardofwor
NoisyNet-Dueling,9149,False,Noisy Networks for Exploration,2017,wizardofwor
Duel noop,7855.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,wizardofwor
DDQN (tuned) noop,7492.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,wizardofwor
A2C + SIL,7088.3,False,Self-Imitation Learning,2018,wizardofwor
Duel hs,7054.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,wizardofwor
Bootstrapped DQN,6804.7,False,Deep Exploration via Bootstrapped DQN,2016,wizardofwor
DDQN (tuned) hs,6201.0,False,Deep Reinforcement Learning with Double Q-learning,2015,wizardofwor
Prior hs,5727.0,False,Prioritized Experience Replay,2015,wizardofwor
A3C FF (1 day) hs,5278.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,wizardofwor
Prior noop,4802.0,False,Prioritized Experience Replay,2015,wizardofwor
POP3D,4704,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,wizardofwor
CGP,3820,False,Evolving simple programs for playing Atari games,2018,wizardofwor
ES FF (1 hour) noop,3480.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,wizardofwor
Nature DQN,3393.0,False,,,wizardofwor
DQN noop,2704.0,False,Deep Reinforcement Learning with Double Q-learning,2015,wizardofwor
Best Learner,1981.3,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,wizardofwor
DQN hs,1609.0,False,Deep Reinforcement Learning with Double Q-learning,2015,wizardofwor
DDQN+Pop-Art noop,483.0,False,Learning values across many orders of magnitude,2016,wizardofwor
SARSA,36.9,False,,,wizardofwor
MuZero,131.13,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,robotank
Agent57,127.32,False,Agent57: Outperforming the Atari Human Benchmark,2020,robotank
GDI-H3(200M frames),113.4,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,robotank
GDI-I3,108.2,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,robotank
MuZero (Res2 Adam),100.59,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,robotank
R2D2,100.4,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,robotank
DreamerV2,78,False,Mastering Atari with Discrete World Models,2020,robotank
FQF,75.7,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,robotank
Ape-X,73.8,False,Distributed Prioritized Experience Replay,2018,robotank
Advantage Learning,69.31,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,robotank
Bootstrapped DQN,66.6,False,Deep Exploration via Bootstrapped DQN,2016,robotank
Duel noop,65.3,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,robotank
DDQN (tuned) noop,65.1,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,robotank
DDQN+Pop-Art noop,64.3,False,Learning values across many orders of magnitude,2016,robotank
NoisyNet-Dueling,64,False,Noisy Networks for Exploration,2017,robotank
DQN noop,63.9,False,Deep Reinforcement Learning with Double Q-learning,2015,robotank
Prior noop,62.6,False,Prioritized Experience Replay,2015,robotank
IQN,62.5,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,robotank
Duel hs,62.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,robotank
Gorila,61.8,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,robotank
QR-DQN-1,59.4,False,Distributional Reinforcement Learning with Quantile Regression,2017,robotank
DDQN (tuned) hs,59.1,False,Deep Reinforcement Learning with Double Q-learning,2015,robotank
DQN hs,58.7,False,Deep Reinforcement Learning with Double Q-learning,2015,robotank
Prior hs,56.2,False,Prioritized Experience Replay,2015,robotank
C51 noop,52.3,False,A Distributional Perspective on Reinforcement Learning,2017,robotank
Nature DQN,51.6,False,,,robotank
UCT,50.4,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,robotank
A3C FF hs,32.8,False,Asynchronous Methods for Deep Reinforcement Learning,2016,robotank
Best Learner,28.7,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,robotank
Prior+Duel noop,27.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,robotank
Prior+Duel hs,24.7,False,Deep Reinforcement Learning with Double Q-learning,2015,robotank
CGP,24.2,False,Evolving simple programs for playing Atari games,2018,robotank
IMPALA (deep),12.96,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,robotank
SARSA,12.4,False,,,robotank
ES FF (1 hour) noop,11.9,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,robotank
A2C + SIL,10.5,False,Self-Imitation Learning,2018,robotank
POP3D,4.6,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,robotank
A3C LSTM hs,2.6,False,Asynchronous Methods for Deep Reinforcement Learning,2016,robotank
A3C FF (1 day) hs,2.3,False,Asynchronous Methods for Deep Reinforcement Learning,2016,robotank
GDI-H3(200M frames),760005,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,asteroids
GDI-I3,751970,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,asteroids
MuZero,678558.64,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,asteroids
MuZero (Res2 Adam),476412,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,asteroids
R2D2,357867.7,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,asteroids
Ape-X,155495.1,False,Distributed Prioritized Experience Replay,2018,asteroids
Agent57,150854.61,False,Agent57: Outperforming the Atari Human Benchmark,2020,asteroids
IMPALA (deep),108590.05,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,asteroids
NoisyNet-Dueling,86700,False,Noisy Networks for Exploration,2017,asteroids
DreamerV2,41526,False,Mastering Atari with Discrete World Models,2020,asteroids
CGP,9412,False,Evolving simple programs for playing Atari games,2018,asteroids
A3C LSTM hs,5093.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,asteroids
UCT,4660.6,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,asteroids
FQF,4553.0,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,asteroids
A3C FF hs,4474.5,False,Asynchronous Methods for Deep Reinforcement Learning,2016,asteroids
QR-DQN-1,4226,False,Distributional Reinforcement Learning with Quantile Regression,2017,asteroids
Reactor 500M,3726.1,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,asteroids
A3C FF (1 day) hs,3009.4,False,Asynchronous Methods for Deep Reinforcement Learning,2016,asteroids
IQN,2898,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,asteroids
DDQN+Pop-Art noop,2869.3,False,Learning values across many orders of magnitude,2016,asteroids
Duel noop,2837.7,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,asteroids
Prior noop,2654.3,False,Prioritized Experience Replay,2015,asteroids
POP3D,2488.1,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,asteroids
A2C + SIL,2259.4,False,Self-Imitation Learning,2018,asteroids
Duel hs,2035.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,asteroids
Advantage Learning,1924.42,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,asteroids
Prior hs,1745.1,False,Prioritized Experience Replay,2015,asteroids
Persistent AL,1673.52,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,asteroids
Nature DQN,1629.0,False,,,asteroids
ES FF (1 hour) noop,1562.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,asteroids
C51 noop,1516.0,False,A Distributional Perspective on Reinforcement Learning,2017,asteroids
DQN hs,1458.7,False,Deep Reinforcement Learning with Double Q-learning,2015,asteroids
DQN noop,1364.5,False,Deep Reinforcement Learning with Double Q-learning,2015,asteroids
DDQN (tuned) hs,1193.2,False,Deep Reinforcement Learning with Double Q-learning,2015,asteroids
Prior+Duel noop,1192.7,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,asteroids
Bootstrapped DQN,1032,False,Deep Exploration via Bootstrapped DQN,2016,asteroids
Prior+Duel hs,1021.9,False,Deep Reinforcement Learning with Double Q-learning,2015,asteroids
Gorila,933.6,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,asteroids
Best Learner,907.3,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,asteroids
DDQN (tuned) noop,734.7,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,asteroids
SARSA,89.0,False,,,asteroids
MuZero,100.00,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,boxing
Ape-X,100,False,Distributed Prioritized Experience Replay,2018,boxing
NoisyNet-Dueling,100,False,Noisy Networks for Exploration,2017,boxing
UCT,100,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,boxing
Agent57,100,False,Agent57: Outperforming the Atari Human Benchmark,2020,boxing
MuZero (Res2 Adam),100,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,boxing
GDI-H3,100,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,boxing
GDI-H3(200M frames),100,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,boxing
IMPALA (deep),99.96,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,boxing
QR-DQN-1,99.9,False,Distributional Reinforcement Learning with Quantile Regression,2017,boxing
IQN,99.8,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,boxing
A2C + SIL,99.6,False,Self-Imitation Learning,2018,boxing
Duel noop,99.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,boxing
Reactor 500M,99.4,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,boxing
DDQN+Pop-Art noop,99.3,False,Learning values across many orders of magnitude,2016,boxing
Prior+Duel noop,98.9,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,boxing
R2D2,98.5,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,boxing
DDRL A3C,98,False,Distributed Deep Reinforcement Learning: Learn how to play Atari games in 21 minutes,2018,boxing
C51 noop,97.8,False,A Distributional Perspective on Reinforcement Learning,2017,boxing
POP3D,97.23,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,boxing
Prior noop,95.6,False,Prioritized Experience Replay,2015,boxing
Persistent AL,94.3,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,boxing
Advantage Learning,93.94,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,boxing
Bootstrapped DQN,93.2,False,Deep Exploration via Bootstrapped DQN,2016,boxing
DreamerV2,92,False,Mastering Atari with Discrete World Models,2020,boxing
DDQN (tuned) noop,91.6,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,boxing
DQN noop,88.0,False,Deep Reinforcement Learning with Double Q-learning,2015,boxing
Prior+Duel hs,79.2,False,Deep Reinforcement Learning with Double Q-learning,2015,boxing
Duel hs,77.3,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,boxing
Gorila,74.2,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,boxing
DDQN (tuned) hs,73.5,False,Deep Reinforcement Learning with Double Q-learning,2015,boxing
Prior hs,72.3,False,Prioritized Experience Replay,2015,boxing
Nature DQN,71.8,False,,,boxing
DQN hs,70.3,False,Deep Reinforcement Learning with Double Q-learning,2015,boxing
A3C FF hs,59.8,False,Asynchronous Methods for Deep Reinforcement Learning,2016,boxing
ES FF (1 hour) noop,49.8,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,boxing
Best Learner,44,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,boxing
CGP,38.4,False,Evolving simple programs for playing Atari games,2018,boxing
A3C LSTM hs,37.3,False,Asynchronous Methods for Deep Reinforcement Learning,2016,boxing
A3C FF (1 day) hs,33.7,False,Asynchronous Methods for Deep Reinforcement Learning,2016,boxing
SARSA,9.8,False,,,boxing
CURL,4.8,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,boxing
NoisyNet-Dueling,10,False,Noisy Networks for Exploration,2017,surround
MuZero,9.99,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,surround
R2D2,9.9,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,surround
MuZero (Res2 Adam),9.9,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,surround
Agent57,9.5,False,Agent57: Outperforming the Atari Human Benchmark,2020,surround
IQN,9.4,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,surround
QR-DQN-1,8.2,False,Distributional Reinforcement Learning with Quantile Regression,2017,surround
IMPALA (deep),7.56,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,surround
Ape-X,7.1,False,Distributed Prioritized Experience Replay,2018,surround
GDI-H3(200M frames),2.606,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,surround
Persistent AL,0.72,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,surround
GDI-I3,-7.8,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,surround
GDI-H3,999999,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,choppercommand
GDI-H3(200M frames),999999,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,choppercommand
Agent57,999900,False,Agent57: Outperforming the Atari Human Benchmark,2020,choppercommand
MuZero,991039.70,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,choppercommand
R2D2,986652.0,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,choppercommand
FQF,876460.0,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,choppercommand
Ape-X,721851,False,Distributed Prioritized Experience Replay,2018,choppercommand
Reactor 500M,107779.0,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,choppercommand
UCT,34018.8,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,choppercommand
IMPALA (deep),28255.00,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,choppercommand
IQN,16836,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,choppercommand
C51 noop,15600.0,False,A Distributional Perspective on Reinforcement Learning,2017,choppercommand
QR-DQN-1,14667,False,Distributional Reinforcement Learning with Quantile Regression,2017,choppercommand
Prior+Duel noop,13185.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,choppercommand
NoisyNet-Dueling,11477,False,Noisy Networks for Exploration,2017,choppercommand
Duel noop,11215.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,choppercommand
A3C LSTM hs,10150.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,choppercommand
Prior noop,8600.0,False,Prioritized Experience Replay,2015,choppercommand
Prior+Duel hs,8058.0,False,Deep Reinforcement Learning with Double Q-learning,2015,choppercommand
A3C FF hs,7021.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,choppercommand
A2C + SIL,6710,False,Self-Imitation Learning,2018,choppercommand
Nature DQN,6687.0,False,,,choppercommand
POP3D,6308.33,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,choppercommand
DQN noop,6126.0,False,Deep Reinforcement Learning with Double Q-learning,2015,choppercommand
MuZero (Res2 Adam),5989.55,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,choppercommand
DDQN (tuned) noop,5809.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,choppercommand
Persistent AL,5734.93,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,choppercommand
Advantage Learning,5431.36,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,choppercommand
DQN hs,5017.0,False,Deep Reinforcement Learning with Double Q-learning,2015,choppercommand
A3C FF (1 day) hs,4669.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,choppercommand
Prior hs,4635.0,False,Prioritized Experience Replay,2015,choppercommand
Bootstrapped DQN,4100,False,Deep Exploration via Bootstrapped DQN,2016,choppercommand
Duel hs,3784.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,choppercommand
ES FF (1 hour) noop,3710.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,choppercommand
CGP,3580,False,Evolving simple programs for playing Atari games,2018,choppercommand
DDQN (tuned) hs,3495.0,False,Deep Reinforcement Learning with Double Q-learning,2015,choppercommand
Gorila,3191.8,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,choppercommand
DreamerV2,2861,False,Mastering Atari with Discrete World Models,2020,choppercommand
Best Learner,1581.5,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,choppercommand
CURL,1198,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,choppercommand
DDQN+Pop-Art noop,775.0,False,Learning values across many orders of magnitude,2016,choppercommand
SARSA,16.9,False,,,choppercommand
GDI-H3(200M frames),154380,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,spaceinvaders
GDI-I3,140460,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,spaceinvaders
MuZero,74335.30,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,spaceinvaders
Ape-X,54681,False,Distributed Prioritized Experience Replay,2018,spaceinvaders
Agent57,48680.86,False,Agent57: Outperforming the Atari Human Benchmark,2020,spaceinvaders
FQF,46498.3,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,spaceinvaders
IMPALA (deep),43595.78,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,spaceinvaders
R2D2,43223.4,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,spaceinvaders
IQN,28888,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,spaceinvaders
A3C LSTM hs,23846.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,spaceinvaders
QR-DQN-1,20972,False,Distributional Reinforcement Learning with Quantile Regression,2017,spaceinvaders
A3C FF hs,15730.5,False,Asynchronous Methods for Deep Reinforcement Learning,2016,spaceinvaders
Prior+Duel noop,15311.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,spaceinvaders
Prior+Duel hs,8978.0,False,Deep Reinforcement Learning with Double Q-learning,2015,spaceinvaders
Duel noop,6427.3,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,spaceinvaders
Duel hs,5993.1,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,spaceinvaders
NoisyNet-Dueling,5909,False,Noisy Networks for Exploration,2017,spaceinvaders
C51 noop,5747.0,False,A Distributional Perspective on Reinforcement Learning,2017,spaceinvaders
Prior hs,3912.1,False,Prioritized Experience Replay,2015,spaceinvaders
MuZero (Res2 Adam),3645.63,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,spaceinvaders
Advantage Learning,3460.79,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,spaceinvaders
Persistent AL,3277.59,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,spaceinvaders
A2C + SIL,2951.7,False,Self-Imitation Learning,2018,spaceinvaders
Bootstrapped DQN,2893,False,Deep Exploration via Bootstrapped DQN,2016,spaceinvaders
Prior noop,2865.8,False,Prioritized Experience Replay,2015,spaceinvaders
UCT,2718,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,spaceinvaders
DDQN (tuned) hs,2628.7,False,Deep Reinforcement Learning with Double Q-learning,2015,spaceinvaders
DDQN+Pop-Art noop,2589.7,False,Learning values across many orders of magnitude,2016,spaceinvaders
DDQN (tuned) noop,2525.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,spaceinvaders
DreamerV2,2474,False,Mastering Atari with Discrete World Models,2020,spaceinvaders
A3C FF (1 day) hs,2214.7,False,Asynchronous Methods for Deep Reinforcement Learning,2016,spaceinvaders
MFEC,1990,False,Model-Free Episodic Control with State Aggregation,2020,spaceinvaders
Nature DQN,1976.0,False,,,spaceinvaders
DQN noop,1692.3,False,Deep Reinforcement Learning with Double Q-learning,2015,spaceinvaders
Recurrent Rational DQN Average,1395,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,spaceinvaders
DQN hs,1293.8,False,Deep Reinforcement Learning with Double Q-learning,2015,spaceinvaders
POP3D,1216.15,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,spaceinvaders
Gorila,1183.3,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,spaceinvaders
MAC,1173.1,False,Mean Actor Critic,2017,spaceinvaders
DQN Best,1075,False,Playing Atari with Deep Reinforcement Learning,2013,spaceinvaders
CGP,1001,False,Evolving simple programs for playing Atari games,2018,spaceinvaders
IDVQ + DRSC + XNES,830,False,Playing Atari with Six Neurons,2018,spaceinvaders
ES FF (1 hour) noop,678.5,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,spaceinvaders
DDRL A3C,650,False,Distributed Deep Reinforcement Learning: Learn how to play Atari games in 21 minutes,2018,spaceinvaders
DARQN soft,650,False,Deep Attention Recurrent Q-Network,2015,spaceinvaders
Rational DQN Average,650,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,spaceinvaders
SARSA,267.9,False,,,spaceinvaders
Best Learner,250.1,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,spaceinvaders
SAC,160.8,False,Soft Actor-Critic for Discrete Action Settings,2019,spaceinvaders
Rainbow,12629.0,False,Rainbow: Combining Improvements in Deep Reinforcement Learning,2017,spaceinvaders
UCT,24,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,doubledunk
GDI-H3,24,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,doubledunk
GDI-H3(200M frames),24,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,doubledunk
MuZero,23.94,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,doubledunk
Agent57,23.93,False,Agent57: Outperforming the Atari Human Benchmark,2020,doubledunk
MuZero (Res2 Adam),23.91,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,doubledunk
R2D2,23.7,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,doubledunk
Ape-X,23.5,False,Distributed Prioritized Experience Replay,2018,doubledunk
Reactor 500M,23.0,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,doubledunk
QR-DQN-1,21.9,False,Distributional Reinforcement Learning with Quantile Regression,2017,doubledunk
A2C + SIL,21.5,False,Self-Imitation Learning,2018,doubledunk
Prior noop,18.5,False,Prioritized Experience Replay,2015,doubledunk
DreamerV2,17,False,Mastering Atari with Discrete World Models,2020,doubledunk
Prior hs,16.0,False,Prioritized Experience Replay,2015,doubledunk
IQN,5.6,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,doubledunk
Bootstrapped DQN,3,False,Deep Exploration via Bootstrapped DQN,2016,doubledunk
C51 noop,2.5,False,A Distributional Perspective on Reinforcement Learning,2017,doubledunk
CGP,2,False,Evolving simple programs for playing Atari games,2018,doubledunk
NoisyNet-Dueling,1,False,Noisy Networks for Exploration,2017,doubledunk
ES FF (1 hour) noop,0.2,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,doubledunk
Duel noop,0.1,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,doubledunk
A3C FF (1 day) hs,0.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,doubledunk
A3C LSTM hs,0.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,doubledunk
SARSA,-16.0,False,,,doubledunk
Nature DQN,-18.1,False,,,doubledunk
Gorila,-11.3,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,doubledunk
DQN hs,-6.0,False,Deep Reinforcement Learning with Double Q-learning,2015,doubledunk
DQN noop,-6.6,False,Deep Reinforcement Learning with Double Q-learning,2015,doubledunk
Duel hs,-0.8,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,doubledunk
DDQN (tuned) noop,-5.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,doubledunk
DDQN (tuned) hs,-0.3,False,Deep Reinforcement Learning with Double Q-learning,2015,doubledunk
Prior+Duel hs,-10.7,False,Deep Reinforcement Learning with Double Q-learning,2015,doubledunk
DDQN+Pop-Art noop,-11.5,False,Learning values across many orders of magnitude,2016,doubledunk
Prior+Duel noop,-12.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,doubledunk
A3C FF hs,-0.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,doubledunk
IMPALA (deep),-0.33,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,doubledunk
Advantage Learning,-0.15,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,doubledunk
Persistent AL,-2.51,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,doubledunk
POP3D,-7.89,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,doubledunk
Best Learner,-13.1,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,doubledunk
Agent57,934134.88,False,Agent57: Outperforming the Atari Human Benchmark,2020,battlezone
MuZero,848623.00,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,battlezone
GDI-H3(200M frames),824360,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,battlezone
R2D2,751880.0,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,battlezone
GDI-I3,478830,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,battlezone
MuZero (Res2 Adam),178716.9,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,battlezone
Ape-X,98895,False,Distributed Prioritized Experience Replay,2018,battlezone
FQF,87928.6,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,battlezone
UCT,70333.3,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,battlezone
Reactor 500M,64070.0,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,battlezone
NoisyNet-Dueling,52262,False,Noisy Networks for Exploration,2017,battlezone
IQN,42244,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,battlezone
DreamerV2,40325,False,Mastering Atari with Discrete World Models,2020,battlezone
QR-DQN-1,39268,False,Distributional Reinforcement Learning with Quantile Regression,2017,battlezone
Bootstrapped DQN,38666.7,False,Deep Exploration via Bootstrapped DQN,2016,battlezone
Duel noop,37150.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,battlezone
Prior+Duel noop,35520.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,battlezone
Persistent AL,34583.07,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,battlezone
CGP,34200,False,Evolving simple programs for playing Atari games,2018,battlezone
DDQN (tuned) noop,31700.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,battlezone
Prior noop,31530.0,False,Prioritized Experience Replay,2015,battlezone
Duel hs,31320.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,battlezone
Prior+Duel hs,30650.0,False,Deep Reinforcement Learning with Double Q-learning,2015,battlezone
DQN noop,29900.0,False,Deep Reinforcement Learning with Double Q-learning,2015,battlezone
Advantage Learning,28789.29,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,battlezone
C51 noop,28742.0,False,A Distributional Perspective on Reinforcement Learning,2017,battlezone
Nature DQN,26300.0,False,,,battlezone
Recurrent Rational DQN Average,25749,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,battlezone
Prior hs,25520.0,False,Prioritized Experience Replay,2015,battlezone
A2C + SIL,25075,False,Self-Imitation Learning,2018,battlezone
DDQN (tuned) hs,24740.0,False,Deep Reinforcement Learning with Double Q-learning,2015,battlezone
DQN hs,23750.0,False,Deep Reinforcement Learning with Double Q-learning,2015,battlezone
Rational DQN Average,23403,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,battlezone
IMPALA (deep),20885.00,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,battlezone
A3C LSTM hs,20760.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,battlezone
Gorila,19938.0,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,battlezone
ES FF (1 hour) noop,16600.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,battlezone
Best Learner,15819.7,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,battlezone
POP3D,15466.67,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,battlezone
A3C FF hs,12950.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,battlezone
A3C FF (1 day) hs,11340.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,battlezone
CURL,11208,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,battlezone
DDQN+Pop-Art noop,8220.0,False,Learning values across many orders of magnitude,2016,battlezone
SAC,4386.7,False,Soft Actor-Critic for Discrete Action Settings,2019,battlezone
SARSA,16.2,False,,,battlezone
R2D2,233413.3,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,kungfumaster
Agent57,206845.82,False,Agent57: Outperforming the Atari Human Benchmark,2020,kungfumaster
MuZero,204824.00,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,kungfumaster
MuZero (Res2 Adam),116726.96,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,kungfumaster
FQF,111138.5,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,kungfumaster
Ape-X,97829.5,False,Distributed Prioritized Experience Replay,2018,kungfumaster
QR-DQN-1,76642,False,Distributional Reinforcement Learning with Quantile Regression,2017,kungfumaster
IQN,73512,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,kungfumaster
DreamerV2,62741,False,Mastering Atari with Discrete World Models,2020,kungfumaster
CGP,57400,False,Evolving simple programs for playing Atari games,2018,kungfumaster
UCT,48854.5,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,kungfumaster
Prior+Duel noop,48375.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,kungfumaster
C51 noop,48192.0,False,A Distributional Perspective on Reinforcement Learning,2017,kungfumaster
IMPALA (deep),43375.50,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,kungfumaster
NoisyNet-Dueling,41672,False,Noisy Networks for Exploration,2017,kungfumaster
A3C LSTM hs,40835.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,kungfumaster
Prior noop,39581.0,False,Prioritized Experience Replay,2015,kungfumaster
Prior+Duel hs,37484.0,False,Deep Reinforcement Learning with Double Q-learning,2015,kungfumaster
Bootstrapped DQN,36733.3,False,Deep Exploration via Bootstrapped DQN,2016,kungfumaster
Persistent AL,34650.91,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,kungfumaster
A2C + SIL,34449.2,False,Self-Imitation Learning,2018,kungfumaster
DDQN+Pop-Art noop,34393.0,False,Learning values across many orders of magnitude,2016,kungfumaster
Duel noop,34294.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,kungfumaster
POP3D,33728,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,kungfumaster
Advantage Learning,32182.99,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,kungfumaster
Prior hs,31676.0,False,Prioritized Experience Replay,2015,kungfumaster
DDQN (tuned) hs,30207.0,False,Deep Reinforcement Learning with Double Q-learning,2015,kungfumaster
DDQN (tuned) noop,29710.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,kungfumaster
SARSA,29151.0,False,,,kungfumaster
A3C FF hs,28819.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,kungfumaster
GDI-H3(200M frames),28349,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,kungfumaster
GDI-I3,28075,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,kungfumaster
DQN noop,26059.0,False,Deep Reinforcement Learning with Double Q-learning,2015,kungfumaster
Duel hs,24288.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,kungfumaster
Nature DQN,23270.0,False,,,kungfumaster
DQN hs,20882.0,False,Deep Reinforcement Learning with Double Q-learning,2015,kungfumaster
Gorila,20620.0,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,kungfumaster
Best Learner,19544,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,kungfumaster
CURL,14280,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,kungfumaster
A3C FF (1 day) hs,3046.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,kungfumaster
Agent57,114736.26,False,Agent57: Outperforming the Atari Human Benchmark,2020,hero
MuZero,49244.11,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,hero
R2D2,39537.1,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,hero
C51 noop,38874,False,A Distributional Perspective on Reinforcement Learning,2017,hero
GDI-I3,38330,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,hero
GDI-H3(200M frames),38225,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,hero
MuZero (Res2 Adam),37234.31,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,hero
IMPALA (deep),33730.55,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,hero
A2C + SIL,33156.7,False,Self-Imitation Learning,2018,hero
A3C FF hs,32464.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,hero
Ape-X,31655.9,False,Distributed Prioritized Experience Replay,2018,hero
NoisyNet-Dueling,31533,False,Noisy Networks for Exploration,2017,hero
FQF,30926.2,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,hero
A3C LSTM hs,28889.5,False,Asynchronous Methods for Deep Reinforcement Learning,2016,hero
A3C FF (1 day) hs,28765.8,False,Asynchronous Methods for Deep Reinforcement Learning,2016,hero
IQN,28386,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,hero
Advantage Learning,24788.86,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,hero
Persistent AL,24175.79,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,hero
Prior noop,23037.7,False,Prioritized Experience Replay,2015,hero
DreamerV2,21868,False,Mastering Atari with Discrete World Models,2020,hero
QR-DQN-1,21395,False,Distributional Reinforcement Learning with Quantile Regression,2017,hero
Prior+Duel noop,21036.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,hero
Bootstrapped DQN,21021.3,False,Deep Exploration via Bootstrapped DQN,2016,hero
Prior hs,20889.9,False,Prioritized Experience Replay,2015,hero
Duel noop,20818.2,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,hero
DQN noop,20437.8,False,Deep Reinforcement Learning with Double Q-learning,2015,hero
DDQN (tuned) noop,20130.2,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,hero
Nature DQN,19950,False,,,hero
Prior+Duel hs,15459.2,False,Deep Reinforcement Learning with Double Q-learning,2015,hero
Duel hs,15207.9,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,hero
DQN hs,14992.9,False,Deep Reinforcement Learning with Double Q-learning,2015,hero
DDQN (tuned) hs,14892.5,False,Deep Reinforcement Learning with Double Q-learning,2015,hero
DDQN+Pop-Art noop,14225.2,False,Learning values across many orders of magnitude,2016,hero
UCT,12859.5,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,hero
MFEC,11732,False,Model-Free Episodic Control with State Aggregation,2020,hero
Gorila,8963.4,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,hero
SARSA,7295,False,,,hero
Best linear,6459,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,hero
Best Learner,6458.8,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,hero
CURL,6235.1,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,hero
CGP,2974,False,Evolving simple programs for playing Atari games,2018,hero
MuZero,143972.03,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,assault
R2D2,108197.0,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,assault
GDI-H3(200M frames),97155,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,assault
Agent57,67212.67,False,Agent57: Outperforming the Atari Human Benchmark,2020,assault
GDI-I3,63876,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,assault
MuZero (Res2 Adam),33292.22,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,assault
IQN,29091,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,assault
Ape-X,24559.4,False,Distributed Prioritized Experience Replay,2018,assault
DreamerV2,23625,False,Mastering Atari with Discrete World Models,2020,assault
QR-DQN-1,22012,False,Distributional Reinforcement Learning with Quantile Regression,2017,assault
IMPALA (deep),19148.47,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,assault
A3C LSTM hs,14497.9,False,Asynchronous Methods for Deep Reinforcement Learning,2016,assault
Prior+Duel noop,11477.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,assault
NoisyNet-Dueling,11231,False,Noisy Networks for Exploration,2017,assault
Prior+Duel hs,10950.6,False,Deep Reinforcement Learning with Double Q-learning,2015,assault
Prior+Duel hs,10950.6,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,assault
DDQN+Pop-Art noop,9011.6,False,Learning values across many orders of magnitude,2016,assault
Reactor 500M,8323.3,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,assault
Bootstrapped DQN,8047.1,False,Deep Exploration via Bootstrapped DQN,2016,assault
Prior noop,7672.1,False,Prioritized Experience Replay,2015,assault
C51 noop,7203.0,False,A Distributional Perspective on Reinforcement Learning,2017,assault
Prior hs,6548.9,False,Prioritized Experience Replay,2015,assault
DDQN (tuned) hs,6060.8,False,Deep Reinforcement Learning with Double Q-learning,2015,assault
A3C FF hs,5474.9,False,Asynchronous Methods for Deep Reinforcement Learning,2016,assault
POP3D,5400.13,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,assault
DDQN (tuned) noop,5393.2,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,assault
Duel noop,4621.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,assault
DQN noop,4280.4,False,Deep Reinforcement Learning with Double Q-learning,2015,assault
Duel hs,3994.8,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,assault
A3C FF (1 day) hs,3746.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,assault
Advantage Learning,3661.51,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,assault
DQN hs,3489.3,False,Deep Reinforcement Learning with Double Q-learning,2015,assault
Nature DQN,3359.0,False,,,assault
Persistent AL,3304.33,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,assault
A2C + SIL,1812,False,Self-Imitation Learning,2018,assault
ES FF (1 hour) noop,1673.9,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,assault
UCT,1512.2,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,assault
Gorila,1195.8,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,assault
CGP,890.4,False,Evolving simple programs for playing Atari games,2018,assault
Best Learner,628,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,assault
CURL,543.7,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,assault
SARSA,537.0,False,,,assault
SAC,350,False,Soft Actor-Critic for Discrete Action Settings,2019,assault
Agent57,24034.16,False,Agent57: Outperforming the Atari Human Benchmark,2020,kangaroo
MuZero,16763.60,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,kangaroo
Prior noop,16200.0,False,Prioritized Experience Replay,2015,kangaroo
IQN,15487,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,kangaroo
QR-DQN-1,15356,False,Distributional Reinforcement Learning with Quantile Regression,2017,kangaroo
NoisyNet-Dueling,15227,False,Noisy Networks for Exploration,2017,kangaroo
Bootstrapped DQN,14862.5,False,Deep Exploration via Bootstrapped DQN,2016,kangaroo
Duel noop,14854.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,kangaroo
GDI-H3(200M frames),14636,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,kangaroo
GDI-I3,14500,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,kangaroo
R2D2,14130.7,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,kangaroo
DreamerV2,14064,False,Mastering Atari with Discrete World Models,2020,kangaroo
MuZero (Res2 Adam),13838,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,kangaroo
DDQN+Pop-Art noop,13150.0,False,Learning values across many orders of magnitude,2016,kangaroo
DDQN (tuned) noop,12992.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,kangaroo
C51 noop,12853.0,False,A Distributional Perspective on Reinforcement Learning,2017,kangaroo
Prior hs,12185.0,False,Prioritized Experience Replay,2015,kangaroo
Persistent AL,11478.46,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,kangaroo
DDQN (tuned) hs,11204.0,False,Deep Reinforcement Learning with Double Q-learning,2015,kangaroo
ES FF (1 hour) noop,11200.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,kangaroo
Advantage Learning,10809.16,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,kangaroo
Duel hs,10334.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,kangaroo
DQN noop,7259.0,False,Deep Reinforcement Learning with Double Q-learning,2015,kangaroo
Nature DQN,6740.0,False,,,kangaroo
Recurrent Rational DQN Average,5266,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,kangaroo
DQN hs,4496.0,False,Deep Reinforcement Learning with Double Q-learning,2015,kangaroo
POP3D,3891.67,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,kangaroo
Rational DQN Average,2941,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,kangaroo
A2C + SIL,2888.3,False,Self-Imitation Learning,2018,kangaroo
UCT,1990,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,kangaroo
Prior+Duel noop,1792.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,kangaroo
IMPALA (deep),1632.00,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,kangaroo
Best Learner,1622.1,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,kangaroo
Gorila,1431.0,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,kangaroo
Ape-X,1416,False,Distributed Prioritized Experience Replay,2018,kangaroo
CGP,1400,False,Evolving simple programs for playing Atari games,2018,kangaroo
IDVQ + DRSC + XNES,1200,False,Playing Atari with Six Neurons,2018,kangaroo
Prior+Duel hs,861.0,False,Deep Reinforcement Learning with Double Q-learning,2015,kangaroo
CURL,345.3,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,kangaroo
A3C LSTM hs,125.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,kangaroo
A3C FF (1 day) hs,106.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,kangaroo
A3C FF hs,94.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,kangaroo
SAC,29.3,False,Soft Actor-Critic for Discrete Action Settings,2019,kangaroo
SARSA,8.8,False,,,kangaroo
GDI-H3(200M frames),3837300,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,atlantis
GDI-I3,3803000,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,atlantis
A2C + SIL,3084781.7,False,Self-Imitation Learning,2018,atlantis
POP3D,2193605.67,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,atlantis
MuZero,1674767.20,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,atlantis
R2D2,1620764.0,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,atlantis
Agent57,1528841.76,False,Agent57: Outperforming the Atari Human Benchmark,2020,atlantis
Persistent AL,1465250,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,atlantis
ES FF (1 hour) noop,1267410.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,atlantis
MuZero (Res2 Adam),1137475.12,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,atlantis
Bootstrapped DQN,994500,False,Deep Exploration via Bootstrapped DQN,2016,atlantis
DreamerV2,978778,False,Mastering Atari with Discrete World Models,2020,atlantis
IQN,978200,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,atlantis
NoisyNet-Dueling,972175,False,Noisy Networks for Exploration,2017,atlantis
QR-DQN-1,971850,False,Distributional Reinforcement Learning with Quantile Regression,2017,atlantis
Ape-X,944497.5,False,Distributed Prioritized Experience Replay,2018,atlantis
A3C FF hs,911091.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,atlantis
A3C LSTM hs,875822.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,atlantis
IMPALA (deep),849967.50,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,atlantis
C51 noop,841075.0,False,A Distributional Perspective on Reinforcement Learning,2017,atlantis
A3C FF (1 day) hs,772392.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,atlantis
Gorila,629166.5,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,atlantis
Advantage Learning,553591.67,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,atlantis
Duel hs,445360.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,atlantis
Prior+Duel hs,423252.0,False,Deep Reinforcement Learning with Double Q-learning,2015,atlantis
Prior+Duel noop,395762.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,atlantis
Duel noop,382572.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,atlantis
Prior noop,357324.0,False,Prioritized Experience Replay,2015,atlantis
DDQN+Pop-Art noop,340076.0,False,Learning values across many orders of magnitude,2016,atlantis
Prior hs,330647.0,False,Prioritized Experience Replay,2015,atlantis
DDQN (tuned) hs,319688.0,False,Deep Reinforcement Learning with Double Q-learning,2015,atlantis
Reactor 500M,302831.0,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,atlantis
DQN hs,292491.0,False,Deep Reinforcement Learning with Double Q-learning,2015,atlantis
DQN noop,279987.0,False,Deep Reinforcement Learning with Double Q-learning,2015,atlantis
UCT,193858,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,atlantis
DDQN (tuned) noop,106056.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,atlantis
CGP,99240,False,Evolving simple programs for playing Atari games,2018,atlantis
Nature DQN,85641.0,False,,,atlantis
Best Learner,62687,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,atlantis
SARSA,852.9,False,,,atlantis
Agent57,839573.53,False,Agent57: Outperforming the Atari Human Benchmark,2020,stargunner
R2D2,717344.0,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,stargunner
GDI-H3(200M frames),677590,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,stargunner
MuZero,549271.70,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,stargunner
GDI-I3,465750,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,stargunner
Ape-X,434342.5,False,Distributed Prioritized Experience Replay,2018,stargunner
IMPALA (deep),200625.00,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,stargunner
A3C LSTM hs,164766.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,stargunner
MuZero (Res2 Adam),154548.26,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,stargunner
A3C FF hs,138218.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,stargunner
FQF,131981.2,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,stargunner
Prior+Duel hs,127073.0,False,Deep Reinforcement Learning with Double Q-learning,2015,stargunner
Prior+Duel noop,125117.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,stargunner
Duel hs,90804.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,stargunner
Duel noop,89238.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,stargunner
QR-DQN-1,77495,False,Distributional Reinforcement Learning with Quantile Regression,2017,stargunner
NoisyNet-Dueling,75867,False,Noisy Networks for Exploration,2017,stargunner
IQN,74677,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,stargunner
RIMs-PPO,70000,False,,,stargunner
A3C FF (1 day) hs,64393.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,stargunner
Prior noop,63302.0,False,Prioritized Experience Replay,2015,stargunner
Prior hs,61582.0,False,Prioritized Experience Replay,2015,stargunner
Advantage Learning,61353.59,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,stargunner
DDQN (tuned) noop,60142.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,stargunner
DDQN (tuned) hs,58365.0,False,Deep Reinforcement Learning with Double Q-learning,2015,stargunner
Nature DQN,57997.0,False,,,stargunner
Bootstrapped DQN,55725,False,Deep Exploration via Bootstrapped DQN,2016,stargunner
DQN noop,54282.0,False,Deep Reinforcement Learning with Double Q-learning,2015,stargunner
DQN hs,52970.0,False,Deep Reinforcement Learning with Double Q-learning,2015,stargunner
C51 noop,49095.0,False,A Distributional Perspective on Reinforcement Learning,2017,stargunner
POP3D,48984,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,stargunner
A2C + SIL,31309.2,False,Self-Imitation Learning,2018,stargunner
Gorila,14919.2,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,stargunner
DreamerV2,7800,False,Mastering Atari with Discrete World Models,2020,stargunner
CGP,2320,False,Evolving simple programs for playing Atari games,2018,stargunner
ES FF (1 hour) noop,1470.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,stargunner
Full Tree,1345,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,stargunner
Best Learner,1069.5,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,stargunner
DDQN+Pop-Art noop,589.0,False,Learning values across many orders of magnitude,2016,stargunner
SARSA,9.4,False,,,stargunner
Go-Explore,43791,False,"First return, then explore",2020,montezumarevenge
Go-Explore,43763,False,Go-Explore: a New Approach for Hard-Exploration Problems,2019,montezumarevenge
Agent57,9352.01,False,Agent57: Outperforming the Atari Human Benchmark,2020,montezumarevenge
RND,8152,False,Exploration by Random Network Distillation,2018,montezumarevenge
A2C+CoEX,6635,False,Contingency-Aware Exploration in Reinforcement Learning,2018,montezumarevenge
DQN-PixelCNN,3705.5,False,Count-Based Exploration with Neural Density Models,2017,montezumarevenge
DDQN-PC,3459,False,Unifying Count-Based Exploration and Intrinsic Motivation,2016,montezumarevenge
GDI-I3,3000,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,montezumarevenge
Sarsa-φ-EB,2745.4,False,Count-Based Exploration in Feature Space for Reinforcement Learning,2017,montezumarevenge
Intrinsic Reward Agent,2504.6,False,Large-Scale Study of Curiosity-Driven Learning,2018,montezumarevenge
Ape-X,2500.0,False,Distributed Prioritized Experience Replay,2018,montezumarevenge
MuZero (Res2 Adam),2500,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,montezumarevenge
GDI-H3(200M frames),2500,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,montezumarevenge
R2D2,2061.3,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,montezumarevenge
DQN+SR,1778.8,False,Count-Based Exploration with the Successor Representation,2018,montezumarevenge
DQNMMCe+SR,1778.6,False,Count-Based Exploration with the Successor Representation,2018,montezumarevenge
A2C + SIL,1100,False,Self-Imitation Learning,2018,montezumarevenge
Sarsa-ε,399.5,False,Count-Based Exploration in Feature Space for Reinforcement Learning,2017,montezumarevenge
A3C-CTS,273.7,False,Unifying Count-Based Exploration and Intrinsic Motivation,2016,montezumarevenge
SARSA,259,False,,,montezumarevenge
MP-EB,142,False,Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models,2015,montezumarevenge
Bootstrapped DQN,100,False,Deep Exploration via Bootstrapped DQN,2016,montezumarevenge
Gorila,84,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,montezumarevenge
DreamerV2,81,False,Mastering Atari with Discrete World Models,2020,montezumarevenge
TRPO-hash,75,False,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning,2016,montezumarevenge
A3C FF hs,67,False,Asynchronous Methods for Deep Reinforcement Learning,2016,montezumarevenge
NoisyNet-Dueling,57,False,Noisy Networks for Exploration,2017,montezumarevenge
A3C FF (1 day) hs,53,False,Asynchronous Methods for Deep Reinforcement Learning,2016,montezumarevenge
Prior hs,51,False,Prioritized Experience Replay,2015,montezumarevenge
DQN hs,47,False,Deep Reinforcement Learning with Double Q-learning,2015,montezumarevenge
DDQN (tuned) hs,42,False,Deep Reinforcement Learning with Double Q-learning,2015,montezumarevenge
A3C LSTM hs,41,False,Asynchronous Methods for Deep Reinforcement Learning,2016,montezumarevenge
Prior+Duel hs,24,False,Deep Reinforcement Learning with Double Q-learning,2015,montezumarevenge
Duel hs,22,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,montezumarevenge
Best Learner,10.7,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,montezumarevenge
Persistent AL,1.72,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,montezumarevenge
Advantage Learning,0.42,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,montezumarevenge
IQN,0,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,montezumarevenge
MuZero,0.00,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,montezumarevenge
IMPALA (deep),0.00,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,montezumarevenge
CGP,0,False,Evolving simple programs for playing Atari games,2018,montezumarevenge
POP3D,0,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,montezumarevenge
QR-DQN-1,0,False,Distributional Reinforcement Learning with Quantile Regression,2017,montezumarevenge
Go-Explore,95756,False,"First return, then explore",2020,privateeye
Agent57,79716.46,False,Agent57: Outperforming the Atari Human Benchmark,2020,privateeye
MuZero,15299.98,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,privateeye
GDI-I3,15100,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,privateeye
GDI-H3(200M frames),15100,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,privateeye
C51 noop,15095.0,False,A Distributional Perspective on Reinforcement Learning,2017,privateeye
CGP,12702.2,False,Evolving simple programs for playing Atari games,2018,privateeye
RND,8666,False,Exploration by Random Network Distillation,2018,privateeye
DQN-PixelCNN,8358.7,False,Count-Based Exploration with Neural Density Models,2017,privateeye
R2D2,5322.7,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,privateeye
Advantage Learning,5276.16,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,privateeye
Intrinsic Reward Agent,3036.5,False,Large-Scale Study of Curiosity-Driven Learning,2018,privateeye
Gorila,2598.6,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,privateeye
DreamerV2,2198,False,Mastering Atari with Discrete World Models,2020,privateeye
Best Baseline,1947.3,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,privateeye
Bootstrapped DQN,1812.5,False,Deep Exploration via Bootstrapped DQN,2016,privateeye
Nature DQN,1788.0,False,,,privateeye
Prior+Duel hs,1277.6,False,Deep Reinforcement Learning with Double Q-learning,2015,privateeye
Best Learner,684.3,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,privateeye
Prior hs,670.7,False,Prioritized Experience Replay,2015,privateeye
A2C + SIL,661.2,False,Self-Imitation Learning,2018,privateeye
A3C LSTM hs,421.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,privateeye
QR-DQN-1,350,False,Distributional Reinforcement Learning with Quantile Regression,2017,privateeye
Duel hs,292.6,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,privateeye
DDQN+Pop-Art noop,286.7,False,Learning values across many orders of magnitude,2016,privateeye
NoisyNet-Dueling,279,False,Noisy Networks for Exploration,2017,privateeye
DQN hs,207.9,False,Deep Reinforcement Learning with Double Q-learning,2015,privateeye
A3C FF hs,206.9,False,Asynchronous Methods for Deep Reinforcement Learning,2016,privateeye
Prior+Duel noop,206.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,privateeye
DQN-CTS,206.0,False,Count-Based Exploration with Neural Density Models,2017,privateeye
Prior noop,200.0,False,Prioritized Experience Replay,2015,privateeye
IQN,200,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,privateeye
A3C FF (1 day) hs,194.4,False,Asynchronous Methods for Deep Reinforcement Learning,2016,privateeye
DQN noop,146.7,False,Deep Reinforcement Learning with Double Q-learning,2015,privateeye
DDQN (tuned) noop,129.7,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,privateeye
CURL,105.2,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,privateeye
Duel noop,103.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,privateeye
ES FF (1 hour) noop,100.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,privateeye
MuZero (Res2 Adam),100,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,privateeye
A3C-CTS,99.32,False,Unifying Count-Based Exploration and Intrinsic Motivation,2016,privateeye
DQNMMCe+SR,99.1,False,Count-Based Exploration with the Successor Representation,2018,privateeye
IMPALA (deep),98.50,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,privateeye
SARSA,86.0,False,,,privateeye
POP3D,79.67,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,privateeye
Ape-X,49.8,False,Distributed Prioritized Experience Replay,2018,privateeye
DDQN (tuned) hs,-575.5,False,Deep Reinforcement Learning with Double Q-learning,2015,privateeye
MuZero,157177.85,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,namethisgame
MuZero (Res2 Adam),101197.71,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,namethisgame
R2D2,58182.7,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,namethisgame
Agent57,54386.77,False,Agent57: Outperforming the Atari Human Benchmark,2020,namethisgame
GDI-H3(200M frames),36296,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,namethisgame
GDI-I3,34434,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,namethisgame
Ape-X,25783.3,False,Distributed Prioritized Experience Replay,2018,namethisgame
IQN,22682,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,namethisgame
QR-DQN-1,21890,False,Distributional Reinforcement Learning with Quantile Regression,2017,namethisgame
IMPALA (deep),21537.20,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,namethisgame
DDQN+Pop-Art noop,15851.2,False,Learning values across many orders of magnitude,2016,namethisgame
Prior+Duel noop,15572.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,namethisgame
UCT,15410,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,namethisgame
A2C + SIL,14958.2,False,Self-Imitation Learning,2018,namethisgame
DreamerV2,14649,False,Mastering Atari with Discrete World Models,2020,namethisgame
Prior+Duel hs,13637.9,False,Deep Reinforcement Learning with Double Q-learning,2015,namethisgame
C51 noop,12542.0,False,A Distributional Perspective on Reinforcement Learning,2017,namethisgame
Prior noop,12270.5,False,Prioritized Experience Replay,2015,namethisgame
NoisyNet-Dueling,12211,False,Noisy Networks for Exploration,2017,namethisgame
A3C LSTM hs,12093.7,False,Asynchronous Methods for Deep Reinforcement Learning,2016,namethisgame
Duel noop,11971.1,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,namethisgame
Bootstrapped DQN,11501.1,False,Deep Exploration via Bootstrapped DQN,2016,namethisgame
Duel hs,11185.1,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,namethisgame
Advantage Learning,11025.26,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,namethisgame
DDQN (tuned) noop,10616.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,namethisgame
Prior hs,10497.6,False,Prioritized Experience Replay,2015,namethisgame
A3C FF hs,10476.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,namethisgame
Persistent AL,10431.33,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,namethisgame
Gorila,9238.5,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,namethisgame
DDQN (tuned) hs,8960.3,False,Deep Reinforcement Learning with Double Q-learning,2015,namethisgame
DQN noop,8207.8,False,Deep Reinforcement Learning with Double Q-learning,2015,namethisgame
Nature DQN,7257.0,False,,,namethisgame
DQN hs,6738.8,False,Deep Reinforcement Learning with Double Q-learning,2015,namethisgame
POP3D,6065.63,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,namethisgame
A3C FF (1 day) hs,5614.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,namethisgame
ES FF (1 hour) noop,4503.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,namethisgame
CGP,3696,False,Evolving simple programs for playing Atari games,2018,namethisgame
Best Learner,2500.1,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,namethisgame
SARSA,2247.0,False,,,namethisgame
IDVQ + DRSC + XNES,920,False,Playing Atari with Six Neurons,2018,namethisgame
Agent57,565909.85,False,Agent57: Outperforming the Atari Human Benchmark,2020,crazyclimber
MuZero,458315.40,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,crazyclimber
R2D2,366690.7,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,crazyclimber
Ape-X,320426,False,Distributed Prioritized Experience Replay,2018,crazyclimber
GDI-H3(200M frames),241170,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,crazyclimber
Reactor 500M,236422.0,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,crazyclimber
FQF,223470.6,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,crazyclimber
GDI-I3,201000,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,crazyclimber
C51 noop,179877.0,False,A Distributional Perspective on Reinforcement Learning,2017,crazyclimber
IQN,179082,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,crazyclimber
NoisyNet-Dueling,171171,False,Noisy Networks for Exploration,2017,crazyclimber
Prior+Duel noop,162224.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,crazyclimber
DreamerV2,161839,False,Mastering Atari with Discrete World Models,2020,crazyclimber
QR-DQN-1,161196,False,Distributional Reinforcement Learning with Quantile Regression,2017,crazyclimber
MuZero (Res2 Adam),158541.58,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,crazyclimber
Duel noop,143570.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,crazyclimber
Prior noop,141161.0,False,Prioritized Experience Replay,2015,crazyclimber
A3C LSTM hs,138518.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,crazyclimber
Bootstrapped DQN,137925.9,False,Deep Exploration via Bootstrapped DQN,2016,crazyclimber
IMPALA (deep),136950.00,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,crazyclimber
A2C + SIL,130185.8,False,Self-Imitation Learning,2018,crazyclimber
Persistent AL,130002.71,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,crazyclimber
Prior+Duel hs,127853.0,False,Deep Reinforcement Learning with Double Q-learning,2015,crazyclimber
Prior hs,127512.0,False,Prioritized Experience Replay,2015,crazyclimber
Duel hs,124566.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,crazyclimber
Advantage Learning,123410.71,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,crazyclimber
POP3D,120247.33,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,crazyclimber
DDQN+Pop-Art noop,119679.0,False,Learning values across many orders of magnitude,2016,crazyclimber
DDQN (tuned) noop,117282.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,crazyclimber
Nature DQN,114103.0,False,,,crazyclimber
DDQN (tuned) hs,113782.0,False,Deep Reinforcement Learning with Double Q-learning,2015,crazyclimber
A3C FF hs,112646.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,crazyclimber
DQN noop,110763.0,False,Deep Reinforcement Learning with Double Q-learning,2015,crazyclimber
A3C FF (1 day) hs,101624.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,crazyclimber
UCT,98172.2,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,crazyclimber
DQN hs,98128.0,False,Deep Reinforcement Learning with Double Q-learning,2015,crazyclimber
Gorila,65451.0,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,crazyclimber
Discrete Latent Space World Model (VQ-VAE),59609.4,False,Smaller World Models for Reinforcement Learning,2020,crazyclimber
VPN,54119,False,Value Prediction Network,2017,crazyclimber
Rainbow+SEER,28066,False,Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings,2021,crazyclimber
CURL,27805.6,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,crazyclimber
ES FF (1 hour) noop,26430.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,crazyclimber
Best Learner,23410.6,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,crazyclimber
CGP,12900,False,Evolving simple programs for playing Atari games,2018,crazyclimber
SAC,3668.7,False,Soft Actor-Critic for Discrete Action Settings,2019,crazyclimber
SARSA,149.8,False,,,crazyclimber
GDI-H3(200M frames),999999,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,roadrunner
GDI-I3,878600,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,roadrunner
MuZero,613411.80,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,roadrunner
R2D2,599246.7,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,roadrunner
MuZero (Res2 Adam),531097,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,roadrunner
Agent57,243025.8,False,Agent57: Outperforming the Atari Human Benchmark,2020,roadrunner
NoisyNet-Dueling,234352,False,Noisy Networks for Exploration,2017,roadrunner
Ape-X,222234.5,False,Distributed Prioritized Experience Replay,2018,roadrunner
DreamerV2,203576,False,Mastering Atari with Discrete World Models,2020,roadrunner
A3C LSTM hs,73949.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,roadrunner
Duel noop,69524.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,roadrunner
QR-DQN-1,64262,False,Distributional Reinforcement Learning with Quantile Regression,2017,roadrunner
Prior+Duel noop,62151.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,roadrunner
Duel hs,58549.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,roadrunner
IQN,57900,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,roadrunner
Prior noop,57608.0,False,Prioritized Experience Replay,2015,roadrunner
IMPALA (deep),57121.00,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,roadrunner
A2C + SIL,57071.7,False,Self-Imitation Learning,2018,roadrunner
C51 noop,55839.0,False,A Distributional Perspective on Reinforcement Learning,2017,roadrunner
Prior+Duel hs,54630.0,False,Deep Reinforcement Learning with Double Q-learning,2015,roadrunner
Advantage Learning,52351.23,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,roadrunner
Prior hs,52264.0,False,Prioritized Experience Replay,2015,roadrunner
Bootstrapped DQN,51500,False,Deep Exploration via Bootstrapped DQN,2016,roadrunner
DDQN+Pop-Art noop,47770.0,False,Learning values across many orders of magnitude,2016,roadrunner
POP3D,44679.67,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,roadrunner
DDQN (tuned) noop,44127.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,roadrunner
DDQN (tuned) hs,43156.0,False,Deep Reinforcement Learning with Double Q-learning,2015,roadrunner
Gorila,43079.8,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,roadrunner
DQN noop,39544.0,False,Deep Reinforcement Learning with Double Q-learning,2015,roadrunner
UCT,38725,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,roadrunner
DQN hs,35215.0,False,Deep Reinforcement Learning with Double Q-learning,2015,roadrunner
A3C FF hs,34216.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,roadrunner
A3C FF (1 day) hs,31769.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,roadrunner
Nature DQN,18257.0,False,,,roadrunner
ES FF (1 hour) noop,16590.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,roadrunner
Rainbow+SEER,11794,False,Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings,2021,roadrunner
CGP,8960,False,Evolving simple programs for playing Atari games,2018,roadrunner
CURL,6786.7,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,roadrunner
SAC,305.3,False,Soft Actor-Critic for Discrete Action Settings,2019,roadrunner
SARSA,89.1,False,,,roadrunner
Best Learner,67.7,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,roadrunner
MuZero,243401.10,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,mspacman
MuZero (Res2 Adam),70659.76,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,mspacman
Agent57,63994.44,False,Agent57: Outperforming the Atari Human Benchmark,2020,mspacman
R2D2,42281.7,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,mspacman
UCT,22336,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,mspacman
GDI-H3(200M frames),11573,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,mspacman
GDI-I3,11536,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,mspacman
Ape-X,11255.2,False,Distributed Prioritized Experience Replay,2018,mspacman
MFEC,8530.4004,False,Model-Free Episodic Control with State Aggregation,2020,mspacman
FQF,7631.9,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,mspacman
IMPALA (deep),7342.32,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,mspacman
Prior noop,6518.7,False,Prioritized Experience Replay,2015,mspacman
IQN,6349,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,mspacman
Duel noop,6283.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,mspacman
QR-DQN-1,5821,False,Distributional Reinforcement Learning with Quantile Regression,2017,mspacman
DreamerV2,5652,False,Mastering Atari with Discrete World Models,2020,mspacman
NoisyNet-Dueling,5546,False,Noisy Networks for Exploration,2017,mspacman
DDQN+Pop-Art noop,4963.8,False,Learning values across many orders of magnitude,2016,mspacman
Advantage Learning,4065.8,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,mspacman
A2C + SIL,4025.1,False,Self-Imitation Learning,2018,mspacman
Persistent AL,3917.55,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,mspacman
C51 noop,3415.0,False,A Distributional Perspective on Reinforcement Learning,2017,mspacman
Prior+Duel noop,3327.3,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,mspacman
DQN noop,3085.6,False,Deep Reinforcement Learning with Double Q-learning,2015,mspacman
Bootstrapped DQN,2983.3,False,Deep Exploration via Bootstrapped DQN,2016,mspacman
DDQN (tuned) noop,2711.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,mspacman
VPN,2689,False,Value Prediction Network,2017,mspacman
CGP,2568,False,Evolving simple programs for playing Atari games,2018,mspacman
Nature DQN,2311.0,False,,,mspacman
Duel hs,2250.6,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,mspacman
Prior hs,1865.9,False,Prioritized Experience Replay,2015,mspacman
Best Learner,1691.8,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,mspacman
POP3D,1683.87,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,mspacman
CURL,1492.8,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,mspacman
Gorila,1263.0,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,mspacman
DDQN (tuned) hs,1241.3,False,Deep Reinforcement Learning with Double Q-learning,2015,mspacman
SARSA,1227.0,False,,,mspacman
DQN hs,1092.3,False,Deep Reinforcement Learning with Double Q-learning,2015,mspacman
Prior+Duel hs,1007.8,False,Deep Reinforcement Learning with Double Q-learning,2015,mspacman
A3C LSTM hs,850.7,False,Asynchronous Methods for Deep Reinforcement Learning,2016,mspacman
SAC,690.9,False,Soft Actor-Critic for Discrete Action Settings,2019,mspacman
A3C FF hs,653.7,False,Asynchronous Methods for Deep Reinforcement Learning,2016,mspacman
A3C FF (1 day) hs,594.4,False,Asynchronous Methods for Deep Reinforcement Learning,2016,mspacman
Rainbow,2570.2,False,Rainbow: Combining Improvements in Deep Reinforcement Learning,2017,mspacman
MuZero,91.16,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,fishingderby
Agent57,86.97,False,Agent57: Outperforming the Atari Human Benchmark,2020,fishingderby
R2D2,85.8,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,fishingderby
MuZero (Res2 Adam),73.94,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,fishingderby
DreamerV2,65,False,Mastering Atari with Discrete World Models,2020,fishingderby
GDI-H3(200M frames),65,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,fishingderby
GDI-I3,59,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,fishingderby
NoisyNet-Dueling,57,False,Noisy Networks for Exploration,2017,fishingderby
A2C + SIL,55.8,False,Self-Imitation Learning,2018,fishingderby
FQF,52.7,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,fishingderby
Duel noop,46.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,fishingderby
DDQN+Pop-Art noop,45.1,False,Learning values across many orders of magnitude,2016,fishingderby
IMPALA (deep),44.85,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,fishingderby
Ape-X,44.4,False,Distributed Prioritized Experience Replay,2018,fishingderby
Prior+Duel noop,41.3,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,fishingderby
Prior noop,39.5,False,Prioritized Experience Replay,2015,fishingderby
QR-DQN-1,39,False,Distributional Reinforcement Learning with Quantile Regression,2017,fishingderby
UCT,37.8,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,fishingderby
IQN,33.8,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,fishingderby
POP3D,28.99,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,fishingderby
Persistent AL,28.13,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,fishingderby
Bootstrapped DQN,26,False,Deep Exploration via Bootstrapped DQN,2016,fishingderby
A3C LSTM hs,22.6,False,Asynchronous Methods for Deep Reinforcement Learning,2016,fishingderby
Advantage Learning,21.32,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,fishingderby
A3C FF hs,18.8,False,Asynchronous Methods for Deep Reinforcement Learning,2016,fishingderby
Prior+Duel hs,17.0,False,Deep Reinforcement Learning with Double Q-learning,2015,fishingderby
DDQN (tuned) noop,15.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,fishingderby
A3C FF (1 day) hs,13.6,False,Asynchronous Methods for Deep Reinforcement Learning,2016,fishingderby
Prior hs,9.8,False,Prioritized Experience Replay,2015,fishingderby
C51 noop,8.9,False,A Distributional Perspective on Reinforcement Learning,2017,fishingderby
Gorila,4.6,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,fishingderby
DDQN (tuned) hs,3.2,False,Deep Reinforcement Learning with Double Q-learning,2015,fishingderby
SARSA,-85.1,False,,,fishingderby
Nature DQN,-0.8,False,,,fishingderby
DQN hs,-1.6,False,Deep Reinforcement Learning with Double Q-learning,2015,fishingderby
DQN noop,-4.9,False,Deep Reinforcement Learning with Double Q-learning,2015,fishingderby
Duel hs,-4.1,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,fishingderby
ES FF (1 hour) noop,-49.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,fishingderby
IDVQ + DRSC + XNES,-10,False,Playing Atari with Six Neurons,2018,fishingderby
CGP,-51,False,Evolving simple programs for playing Atari games,2018,fishingderby
Best Learner,-89.5,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,fishingderby
R2D2,999383.2,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,videopinball
Agent57,992340.74,False,Agent57: Outperforming the Atari Human Benchmark,2020,videopinball
MuZero,981791.88,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,videopinball
GDI-H3(200M frames),978190,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,videopinball
C51 noop,949604.0,False,A Distributional Perspective on Reinforcement Learning,2017,videopinball
GDI-I3,925830,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,videopinball
NoisyNet-Dueling,870954,False,Noisy Networks for Exploration,2017,videopinball
MuZero (Res2 Adam),865543.44,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,videopinball
Bootstrapped DQN,811610,False,Deep Exploration via Bootstrapped DQN,2016,videopinball
QR-DQN-1,705662,False,Distributional Reinforcement Learning with Quantile Regression,2017,videopinball
IQN,698045,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,videopinball
IMPALA (deep),572898.27,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,videopinball
Ape-X,565163.2,False,Distributed Prioritized Experience Replay,2018,videopinball
Advantage Learning,543504,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,videopinball
Prior+Duel noop,479197.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,videopinball
A3C LSTM hs,470310.5,False,Asynchronous Methods for Deep Reinforcement Learning,2016,videopinball
A2C + SIL,461522.4,False,Self-Imitation Learning,2018,videopinball
Prior+Duel hs,447408.6,False,Deep Reinforcement Learning with Double Q-learning,2015,videopinball
DDQN (tuned) hs,367823.7,False,Deep Reinforcement Learning with Double Q-learning,2015,videopinball
A3C FF hs,331628.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,videopinball
DDQN (tuned) noop,309941.9,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,videopinball
Prior hs,295972.8,False,Prioritized Experience Replay,2015,videopinball
Prior noop,282007.3,False,Prioritized Experience Replay,2015,videopinball
UCT,254748,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,videopinball
DQN noop,196760.4,False,Deep Reinforcement Learning with Double Q-learning,2015,videopinball
A3C FF (1 day) hs,185852.6,False,Asynchronous Methods for Deep Reinforcement Learning,2016,videopinball
DQN hs,154414.1,False,Deep Reinforcement Learning with Double Q-learning,2015,videopinball
Rational DQN Average,149712,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,videopinball
Gorila,112093.4,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,videopinball
Duel hs,110976.2,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,videopinball
Duel noop,98209.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,videopinball
Recurrent Rational DQN Average,86942,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,videopinball
DDQN+Pop-Art noop,56287.0,False,Learning values across many orders of magnitude,2016,videopinball
Nature DQN,42684.0,False,,,videopinball
DreamerV2,41860,False,Mastering Atari with Discrete World Models,2020,videopinball
POP3D,37780.7,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,videopinball
CGP,33752.4,False,Evolving simple programs for playing Atari games,2018,videopinball
ES FF (1 hour) noop,22834.8,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,videopinball
SARSA,19761.0,False,,,videopinball
Best Learner,16871.3,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,videopinball
MuZero,864.00,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,breakout
GDI-H3,864,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,breakout
GDI-H3(200M frames),864,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,breakout
Bootstrapped DQN,855,False,Deep Exploration via Bootstrapped DQN,2016,breakout
FQF,854.2,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,breakout
R2D2,837.7,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,breakout
Ape-X,800.9,False,Distributed Prioritized Experience Replay,2018,breakout
Agent57,790.4,False,Agent57: Outperforming the Atari Human Benchmark,2020,breakout
IMPALA (deep),787.34,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,breakout
A3C LSTM hs,766.8,False,Asynchronous Methods for Deep Reinforcement Learning,2016,breakout
MuZero (Res2 Adam),758.04,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,breakout
C51 noop,748.0,False,A Distributional Perspective on Reinforcement Learning,2017,breakout
QR-DQN-1,742,False,Distributional Reinforcement Learning with Quantile Regression,2017,breakout
IQN,734,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,breakout
A3C FF hs,681.9,False,Asynchronous Methods for Deep Reinforcement Learning,2016,breakout
A3C FF (1 day) hs,551.6,False,Asynchronous Methods for Deep Reinforcement Learning,2016,breakout
Reactor 500M,514.8,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,breakout
POP3D,458.41,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,breakout
A2C + SIL,452,False,Self-Imitation Learning,2018,breakout
Persistent AL,431.89,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,breakout
Advantage Learning,425.32,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,breakout
DDQN (tuned) noop,418.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,breakout
Duel hs,411.6,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,breakout
Nature DQN,401.2,False,,,breakout
DQN noop,385.5,False,Deep Reinforcement Learning with Double Q-learning,2015,breakout
Prior noop,373.9,False,Prioritized Experience Replay,2015,breakout
MAC,372.7,False,Mean Actor Critic,2017,breakout
DDQN (tuned) hs,368.9,False,Deep Reinforcement Learning with Double Q-learning,2015,breakout
Prior+Duel noop,366.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,breakout
UCT,364.4,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,breakout
Prior+Duel hs,354.6,False,Deep Reinforcement Learning with Double Q-learning,2015,breakout
DQN hs,354.5,False,Deep Reinforcement Learning with Double Q-learning,2015,breakout
DDRL A3C,350,False,Distributed Deep Reinforcement Learning: Learn how to play Atari games in 21 minutes,2018,breakout
Duel noop,345.3,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,breakout
DDQN+Pop-Art noop,344.1,False,Learning values across many orders of magnitude,2016,breakout
Prior hs,343.0,False,Prioritized Experience Replay,2015,breakout
Recurrent Rational DQN Average,336,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,breakout
Rational DQN Average,316,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,breakout
Gorila,313.0,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,breakout
DreamerV2,312,False,Mastering Atari with Discrete World Models,2020,breakout
DT,267.5,False,Decision Transformer: Reinforcement Learning via Sequence Modeling,2021,breakout
NoisyNet-Dueling,263,False,Noisy Networks for Exploration,2017,breakout
DQN Best,225,False,Playing Atari with Deep Reinforcement Learning,2013,breakout
SPOS,180.6,False,Optimizing the Neural Architecture of Reinforcement Learning Agents,2020,breakout
ENAS Search space 1,161.1,False,Optimizing the Neural Architecture of Reinforcement Learning Agents,2020,breakout
SPOS Search space 1,144.4,False,Optimizing the Neural Architecture of Reinforcement Learning Agents,2020,breakout
ENAS,91.4,False,Optimizing the Neural Architecture of Reinforcement Learning Agents,2020,breakout
DARQN hard,20,False,Deep Attention Recurrent Q-Network,2015,breakout
CURL,18.2,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,breakout
CGP,13.2,False,Evolving simple programs for playing Atari games,2018,breakout
Discrete Latent Space World Model (VQ-VAE),11.6,False,Smaller World Models for Reinforcement Learning,2020,breakout
ES FF (1 hour) noop,9.5,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,breakout
SARSA,6.1,False,,,breakout
Best Learner,5.2,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,breakout
SAC,0.7,False,Soft Actor-Critic for Discrete Action Settings,2019,breakout
GDI-H3(200M frames),594540,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,krull
MuZero,269358.27,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,krull
Agent57,251997.31,False,Agent57: Outperforming the Atari Human Benchmark,2020,krull
R2D2,218448.1,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,krull
GDI-I3,97575,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,krull
MuZero (Res2 Adam),72570.5,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,krull
DreamerV2,50061,False,Mastering Atari with Discrete World Models,2020,krull
VPN,15930,False,Value Prediction Network,2017,krull
Ape-X,11741.4,False,Distributed Prioritized Experience Replay,2018,krull
Duel noop,11451.9,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,krull
QR-DQN-1,11447,False,Distributional Reinforcement Learning with Quantile Regression,2017,krull
NoisyNet-Dueling,10754,False,Noisy Networks for Exploration,2017,krull
IQN,10707,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,krull
A2C + SIL,10614.6,False,Self-Imitation Learning,2018,krull
Prior+Duel noop,10374.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,krull
DDQN+Pop-Art noop,9745.1,False,Learning values across many orders of magnitude,2016,krull
C51 noop,9735.0,False,A Distributional Perspective on Reinforcement Learning,2017,krull
Prior noop,9728.0,False,Prioritized Experience Replay,2015,krull
Advantage Learning,9548.92,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,krull
CGP,9086.8,False,Evolving simple programs for playing Atari games,2018,krull
Persistent AL,8689.81,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,krull
ES FF (1 hour) noop,8647.2,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,krull
Bootstrapped DQN,8627.9,False,Deep Exploration via Bootstrapped DQN,2016,krull
DQN noop,8422.3,False,Deep Reinforcement Learning with Double Q-learning,2015,krull
IMPALA (deep),8147.40,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,krull
A3C FF (1 day) hs,8066.6,False,Asynchronous Methods for Deep Reinforcement Learning,2016,krull
Duel hs,8051.6,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,krull
DDQN (tuned) noop,7920.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,krull
POP3D,7715.68,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,krull
Prior+Duel hs,7658.6,False,Deep Reinforcement Learning with Double Q-learning,2015,krull
Prior hs,6872.8,False,Prioritized Experience Replay,2015,krull
DDQN (tuned) hs,6796.1,False,Deep Reinforcement Learning with Double Q-learning,2015,krull
Gorila,6363.1,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,krull
DQN hs,6206.0,False,Deep Reinforcement Learning with Double Q-learning,2015,krull
A3C LSTM hs,5911.4,False,Asynchronous Methods for Deep Reinforcement Learning,2016,krull
A3C FF hs,5560.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,krull
UCT,5037,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,krull
CURL,3833.6,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,krull
Nature DQN,3805.0,False,,,krull
Best Learner,3371.5,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,krull
SARSA,3341.0,False,,,krull
Rainbow+SEER,3277.5,False,Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings,2021,krull
Go-Explore,1422628,False,"First return, then explore",2020,centipede
MuZero,1159049.27,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,centipede
MuZero (Res2 Adam),874301.64,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,centipede
R2D2,599140.3,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,centipede
Agent57,412847.86,False,Agent57: Outperforming the Atari Human Benchmark,2020,centipede
GDI-H3(200M frames),195630,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,centipede
GDI-I3,155830,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,centipede
Full Tree,125123,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,centipede
DDQN+Pop-Art noop,49065.8,False,Learning values across many orders of magnitude,2016,centipede
CGP,24708,False,Evolving simple programs for playing Atari games,2018,centipede
Ape-X,12974,False,Distributed Prioritized Experience Replay,2018,centipede
QR-DQN-1,12447,False,Distributional Reinforcement Learning with Quantile Regression,2017,centipede
DreamerV2,11883,False,Mastering Atari with Discrete World Models,2020,centipede
IQN,11561,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,centipede
IMPALA (deep),11049.75,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,centipede
C51 noop,9646.0,False,A Distributional Perspective on Reinforcement Learning,2017,centipede
Best Learner,8803.8,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,centipede
Nature DQN,8309.0,False,,,centipede
ES FF (1 hour) noop,7783.9,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,centipede
Prior+Duel noop,7687.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,centipede
NoisyNet-Dueling,7596,False,Noisy Networks for Exploration,2017,centipede
Duel noop,7561.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,centipede
A2C + SIL,7559.5,False,Self-Imitation Learning,2018,centipede
Gorila,6296.9,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,centipede
Prior+Duel hs,5570.2,False,Deep Reinforcement Learning with Double Q-learning,2015,centipede
DDQN (tuned) noop,5409.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,centipede
Duel hs,4881.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,centipede
DQN noop,4657.7,False,Deep Reinforcement Learning with Double Q-learning,2015,centipede
SARSA,4647.0,False,,,centipede
Bootstrapped DQN,4553.5,False,Deep Exploration via Bootstrapped DQN,2016,centipede
Persistent AL,4539.55,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,centipede
Prior noop,4463.2,False,Prioritized Experience Replay,2015,centipede
Advantage Learning,4225.18,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,centipede
DQN hs,3973.9,False,Deep Reinforcement Learning with Double Q-learning,2015,centipede
DDQN (tuned) hs,3853.5,False,Deep Reinforcement Learning with Double Q-learning,2015,centipede
A3C FF hs,3755.8,False,Asynchronous Methods for Deep Reinforcement Learning,2016,centipede
Prior hs,3489.1,False,Prioritized Experience Replay,2015,centipede
Reactor 500M,3422.0,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,centipede
POP3D,3315.44,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,centipede
A3C FF (1 day) hs,3306.5,False,Asynchronous Methods for Deep Reinforcement Learning,2016,centipede
A3C LSTM hs,1997.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,centipede
MuZero,631378.53,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,frostbite
Agent57,541280.88,False,Agent57: Outperforming the Atari Human Benchmark,2020,frostbite
MuZero (Res2 Adam),374769.76,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,frostbite
R2D2,315456.4,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,frostbite
FQF,16472.9,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,frostbite
DreamerV2,11384,False,Mastering Atari with Discrete World Models,2020,frostbite
GDI-H3(200M frames),11330,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,frostbite
GDI-I3,10485,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,frostbite
Ape-X,9328.6,False,Distributed Prioritized Experience Replay,2018,frostbite
Prior+Duel noop,7413.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,frostbite
A2C + SIL,6289.8,False,Self-Imitation Learning,2018,frostbite
TRPO-hash,5214.0,False,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning,2016,frostbite
Duel noop,4672.8,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,frostbite
QR-DQN-1,4384,False,Distributional Reinforcement Learning with Quantile Regression,2017,frostbite
Prior noop,4380.1,False,Prioritized Experience Replay,2015,frostbite
IQN,4324,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,frostbite
Prior+Duel hs,4038.4,False,Deep Reinforcement Learning with Double Q-learning,2015,frostbite
C51 noop,3965.0,False,A Distributional Perspective on Reinforcement Learning,2017,frostbite
VPN,3811,False,Value Prediction Network,2017,frostbite
Prior hs,3510.0,False,Prioritized Experience Replay,2015,frostbite
DDQN+Pop-Art noop,3469.6,False,Learning values across many orders of magnitude,2016,frostbite
Persistent AL,3248.96,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,frostbite
NoisyNet-Dueling,2923,False,Noisy Networks for Exploration,2017,frostbite
Sarsa-φ-EB,2770.1,False,Count-Based Exploration in Feature Space for Reinforcement Learning,2017,frostbite
MFEC,2394,False,Model-Free Episodic Control with State Aggregation,2020,frostbite
Duel hs,2332.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,frostbite
Advantage Learning,2305.82,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,frostbite
Bootstrapped DQN,2181.4,False,Deep Exploration via Bootstrapped DQN,2016,frostbite
DDQN (tuned) noop,1683.3,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,frostbite
DDQN (tuned) hs,1448.1,False,Deep Reinforcement Learning with Double Q-learning,2015,frostbite
Sarsa-ε,1394.3,False,Count-Based Exploration in Feature Space for Reinforcement Learning,2017,frostbite
CURL,924,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,frostbite
DQN noop,797.4,False,Deep Reinforcement Learning with Double Q-learning,2015,frostbite
CGP,782,False,Evolving simple programs for playing Atari games,2018,frostbite
MP-EB,507.0,False,Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models,2015,frostbite
DQN hs,496.1,False,Deep Reinforcement Learning with Double Q-learning,2015,frostbite
Gorila,426.6,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,frostbite
ES FF (1 hour) noop,370.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,frostbite
Nature DQN,328.3,False,,,frostbite
IMPALA (deep),317.75,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,frostbite
POP3D,316.87,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,frostbite
IDVQ + DRSC + XNES,300,False,Playing Atari with Six Neurons,2018,frostbite
UCT,270.5,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,frostbite
Best Learner,216.9,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,frostbite
A3C LSTM hs,197.6,False,Asynchronous Methods for Deep Reinforcement Learning,2016,frostbite
A3C FF hs,190.5,False,Asynchronous Methods for Deep Reinforcement Learning,2016,frostbite
SARSA,180.9,False,,,frostbite
A3C FF (1 day) hs,180.1,False,Asynchronous Methods for Deep Reinforcement Learning,2016,frostbite
SAC,59.4,False,Soft Actor-Critic for Discrete Action Settings,2019,frostbite
MuZero,476763.90,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,timepilot
GDI-H3(200M frames),450810,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,timepilot
R2D2,445377.3,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,timepilot
MuZero (Res2 Adam),424011.16,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,timepilot
Agent57,405425.31,False,Agent57: Outperforming the Atari Human Benchmark,2020,timepilot
GDI-I3,216770,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,timepilot
Ape-X,87085,False,Distributed Prioritized Experience Replay,2018,timepilot
UCT,63854.5,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,timepilot
IMPALA (deep),48481.50,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,timepilot
DreamerV2,37945,False,Mastering Atari with Discrete World Models,2020,timepilot
A3C LSTM hs,27202.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,timepilot
Rational DQN Average,17632,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,timepilot
NoisyNet-Dueling,17301,False,Noisy Networks for Exploration,2017,timepilot
Recurrent Rational DQN Average,13261,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,timepilot
A3C FF hs,12679.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,timepilot
IQN,12236,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,timepilot
CGP,12040,False,Evolving simple programs for playing Atari games,2018,timepilot
Duel noop,11666.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,timepilot
A2C + SIL,10811.7,False,Self-Imitation Learning,2018,timepilot
QR-DQN-1,10345,False,Distributional Reinforcement Learning with Quantile Regression,2017,timepilot
Prior noop,9197.0,False,Prioritized Experience Replay,2015,timepilot
Bootstrapped DQN,9079.4,False,Deep Exploration via Bootstrapped DQN,2016,timepilot
Advantage Learning,8969.12,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,timepilot
DDQN (tuned) noop,8339.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,timepilot
C51 noop,8329.0,False,A Distributional Perspective on Reinforcement Learning,2017,timepilot
Gorila,8267.8,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,timepilot
Prior+Duel noop,7553.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,timepilot
DDQN (tuned) hs,6608.0,False,Deep Reinforcement Learning with Double Q-learning,2015,timepilot
Duel hs,6601.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,timepilot
Prior hs,5963.0,False,Prioritized Experience Replay,2015,timepilot
Nature DQN,5947.0,False,,,timepilot
A3C FF (1 day) hs,5825.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,timepilot
ES FF (1 hour) noop,4970.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,timepilot
Prior+Duel hs,4871.0,False,Deep Reinforcement Learning with Double Q-learning,2015,timepilot
DQN noop,4870.0,False,Deep Reinforcement Learning with Double Q-learning,2015,timepilot
DDQN+Pop-Art noop,4870.0,False,Learning values across many orders of magnitude,2016,timepilot
DQN hs,4786.0,False,Deep Reinforcement Learning with Double Q-learning,2015,timepilot
IDVQ + DRSC + XNES,4600,False,Playing Atari with Six Neurons,2018,timepilot
POP3D,3770.33,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,timepilot
Best Learner,3741.2,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,timepilot
SARSA,24.9,False,,,timepilot
Best Learner,0,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,skiing
Full Tree,0,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,skiing
IQN,-9289,True,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,skiing
MuZero,-29968.36,True,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,skiing
R2D2,-30021.7,True,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,skiing
IMPALA (deep),-10180.38,True,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,skiing
FQF,-9085.3,True,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,skiing
NoisyNet-Dueling,-7550,True,Noisy Networks for Exploration,2017,skiing
CGP,-9011,True,Evolving simple programs for playing Atari games,2018,skiing
QR-DQN-1,-9324,True,Distributional Reinforcement Learning with Quantile Regression,2017,skiing
Ape-X,-10789.9,True,Distributed Prioritized Experience Replay,2018,skiing
Advantage Learning,-13264.51,True,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,skiing
Go-Explore,-3660,True,"First return, then explore",2020,skiing
Agent57,-4202.6,True,Agent57: Outperforming the Atari Human Benchmark,2020,skiing
DreamerV2,-9299,True,Mastering Atari with Discrete World Models,2020,skiing
Recurrent Rational DQN Average,-23582,True,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,skiing
Rational DQN Average,-23487,True,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,skiing
MuZero (Res2 Adam),-30000,True,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,skiing
GDI-I3,-6774,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,skiing
GDI-H3(200M frames),-6025,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,skiing
Go-Explore,197376,False,"First return, then explore",2020,berzerk
MuZero,85932.60,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,berzerk
Agent57,61507.83,False,Agent57: Outperforming the Atari Human Benchmark,2020,berzerk
Ape-X,57196.7,False,Distributed Prioritized Experience Replay,2018,berzerk
R2D2,53318.7,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,berzerk
GDI-H3(200M frames),14649,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,berzerk
FQF,12422.2,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,berzerk
GDI-I3,7607,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,berzerk
Prior+Duel noop,3409.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,berzerk
QR-DQN-1,3117,False,Distributional Reinforcement Learning with Quantile Regression,2017,berzerk
MuZero (Res2 Adam),2705.82,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,berzerk
Reactor 500M,2303.1,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,berzerk
Prior+Duel hs,2178.6,False,Deep Reinforcement Learning with Double Q-learning,2015,berzerk
NoisyNet-Dueling,1896,False,Noisy Networks for Exploration,2017,berzerk
IMPALA (deep),1852.70,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,berzerk
C51 noop,1645.0,False,A Distributional Perspective on Reinforcement Learning,2017,berzerk
Duel noop,1472.6,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,berzerk
A3C FF (1 day) hs,1433.4,False,Asynchronous Methods for Deep Reinforcement Learning,2016,berzerk
Persistent AL,1328.25,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,berzerk
Prior noop,1305.6,False,Prioritized Experience Replay,2015,berzerk
DDQN (tuned) noop,1225.4,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,berzerk
DDQN+Pop-Art noop,1199.6,False,Learning values across many orders of magnitude,2016,berzerk
CGP,1138,False,Evolving simple programs for playing Atari games,2018,berzerk
IQN,1053,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,berzerk
DDQN (tuned) hs,1011.1,False,Deep Reinforcement Learning with Double Q-learning,2015,berzerk
Duel hs,910.6,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,berzerk
Prior hs,865.9,False,Prioritized Experience Replay,2015,berzerk
A3C LSTM hs,862.2,False,Asynchronous Methods for Deep Reinforcement Learning,2016,berzerk
A3C FF hs,817.9,False,Asynchronous Methods for Deep Reinforcement Learning,2016,berzerk
DreamerV2,810,False,Mastering Atari with Discrete World Models,2020,berzerk
Advantage Learning,747.26,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,berzerk
ES FF (1 hour) noop,686.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,berzerk
Best Baseline,670,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,berzerk
DQN noop,585.6,False,Deep Reinforcement Learning with Double Q-learning,2015,berzerk
Best Learner,501.3,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,berzerk
DQN hs,493.4,False,Deep Reinforcement Learning with Double Q-learning,2015,berzerk
Agent57,2623.71,False,Agent57: Outperforming the Atari Human Benchmark,2020,venture
Go-Explore,2281,False,"First return, then explore",2020,venture
GDI-I3,2035,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,venture
GDI-H3(200M frames),2000,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,venture
R2D2,1970.7,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,venture
RND,1859,False,Exploration by Random Network Distillation,2018,venture
Ape-X,1813,False,Distributed Prioritized Experience Replay,2018,venture
MuZero (Res2 Adam),1731.47,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,venture
C51 noop,1520.0,False,A Distributional Perspective on Reinforcement Learning,2017,venture
RUDDER,1350,False,RUDDER: Return Decomposition for Delayed Rewards,2018,venture
IQN,1318,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,venture
DQNMMCe+SR,1241.8,False,Count-Based Exploration with the Successor Representation,2018,venture
DDQN+Pop-Art noop,1172.0,False,Learning values across many orders of magnitude,2016,venture
Sarsa-φ-EB,1169.2,False,Count-Based Exploration in Feature Space for Reinforcement Learning,2017,venture
NoisyNet-Dueling,815,False,Noisy Networks for Exploration,2017,venture
ES FF (1 hour) noop,760.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,venture
Gorila,523.4,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,venture
Duel noop,497.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,venture
TRPO-hash,445.0,False,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning,2016,venture
Intrinsic Reward Agent,416,False,Large-Scale Study of Curiosity-Driven Learning,2018,venture
Nature DQN,380.0,False,,,venture
Bootstrapped DQN,212.5,False,Deep Exploration via Bootstrapped DQN,2016,venture
Duel hs,200.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,venture
Advantage Learning,198.69,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,venture
DQN noop,163.0,False,Deep Reinforcement Learning with Double Q-learning,2015,venture
DQN hs,136.0,False,Deep Reinforcement Learning with Double Q-learning,2015,venture
DDQN (tuned) noop,98.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,venture
Prior hs,94.0,False,Prioritized Experience Replay,2015,venture
DQN-PixelCNN,82.2,False,Count-Based Exploration with Neural Density Models,2017,venture
Best Learner,66,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,venture
Prior noop,54.0,False,Prioritized Experience Replay,2015,venture
Prior+Duel noop,48.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,venture
DQN-CTS,48.0,False,Count-Based Exploration with Neural Density Models,2017,venture
QR-DQN-1,43.9,False,Distributional Reinforcement Learning with Quantile Regression,2017,venture
POP3D,36.33,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,venture
Prior+Duel hs,29.0,False,Deep Reinforcement Learning with Double Q-learning,2015,venture
A3C LSTM hs,25.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,venture
A3C FF hs,23.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,venture
DDQN (tuned) hs,21.0,False,Deep Reinforcement Learning with Double Q-learning,2015,venture
A3C FF (1 day) hs,19.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,venture
DreamerV2,2,False,Mastering Atari with Discrete World Models,2020,venture
SARSA,0.6,False,,,venture
MuZero,0.40,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,venture
MP-EB,0.0,False,Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models,2015,venture
A3C-CTS,0.0,False,Unifying Count-Based Exploration and Intrinsic Motivation,2016,venture
Sarsa-ε,0.0,False,Count-Based Exploration in Feature Space for Reinforcement Learning,2017,venture
IMPALA (deep),0.00,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,venture
A2C + SIL,0,False,Self-Imitation Learning,2018,venture
CGP,0,False,Evolving simple programs for playing Atari games,2018,venture
MuZero,454993.53,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,beamrider
GDI-H3(200M frames),422390,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,beamrider
MuZero (Res2 Adam),333077.44,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,beamrider
Agent57,300509.8,False,Agent57: Outperforming the Atari Human Benchmark,2020,beamrider
R2D2,188257.4,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,beamrider
GDI-I3,162100,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,beamrider
Ape-X,63305.2,False,Distributed Prioritized Experience Replay,2018,beamrider
IQN,42776,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,beamrider
Prior+Duel hs,37412.2,False,Deep Reinforcement Learning with Double Q-learning,2015,beamrider
QR-DQN-1,34821,False,Distributional Reinforcement Learning with Quantile Regression,2017,beamrider
IMPALA (deep),32463.47,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,beamrider
Prior hs,31181.3,False,Prioritized Experience Replay,2015,beamrider
Prior+Duel noop,30276.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,beamrider
A3C LSTM hs,24622.2,False,Asynchronous Methods for Deep Reinforcement Learning,2016,beamrider
Bootstrapped DQN,23429.8,False,Deep Exploration via Bootstrapped DQN,2016,beamrider
Prior noop,23384.2,False,Prioritized Experience Replay,2015,beamrider
NoisyNet-Dueling,23134,False,Noisy Networks for Exploration,2017,beamrider
A3C FF hs,22707.9,False,Asynchronous Methods for Deep Reinforcement Learning,2016,beamrider
DreamerV2,18646,False,Mastering Atari with Discrete World Models,2020,beamrider
DDQN (tuned) hs,17417.2,False,Deep Reinforcement Learning with Double Q-learning,2015,beamrider
DDRL A3C,14900,False,Distributed Deep Reinforcement Learning: Learn how to play Atari games in 21 minutes,2018,beamrider
Duel hs,14591.3,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,beamrider
C51 noop,14074.0,False,A Distributional Perspective on Reinforcement Learning,2017,beamrider
DDQN (tuned) noop,13772.8,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,beamrider
A3C FF (1 day) hs,13235.9,False,Asynchronous Methods for Deep Reinforcement Learning,2016,beamrider
Persistent AL,13145.34,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,beamrider
Duel noop,12164.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,beamrider
Reactor 500M,11033.4,False,The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,2017,beamrider
Advantage Learning,10054.58,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,beamrider
DQN hs,9743.2,False,Deep Reinforcement Learning with Double Q-learning,2015,beamrider
DQN noop,8627.5,False,Deep Reinforcement Learning with Double Q-learning,2015,beamrider
DDQN+Pop-Art noop,8299.4,False,Learning values across many orders of magnitude,2016,beamrider
Nature DQN,6846.0,False,,,beamrider
UCT,6624.6,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,beamrider
MAC,6072,False,Mean Actor Critic,2017,beamrider
RIMs-PPO,5320,False,,,beamrider
DQN Best,5184,False,Playing Atari with Deep Reinforcement Learning,2013,beamrider
POP3D,4549,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,beamrider
Gorila,3822.1,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,beamrider
A2C + SIL,2366.2,False,Self-Imitation Learning,2018,beamrider
SARSA,1743.0,False,,,beamrider
CGP,1341.6,False,Evolving simple programs for playing Atari games,2018,beamrider
Best Learner,929.4,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,beamrider
ES FF (1 hour) noop,744.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,beamrider
SAC,432.1,False,Soft Actor-Critic for Discrete Action Settings,2019,beamrider
GDI-H3(200M frames),620780,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,jamesbond
GDI-I3,594500,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,jamesbond
Agent57,135784.96,False,Agent57: Outperforming the Atari Human Benchmark,2020,jamesbond
FQF,87291.7,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,jamesbond
MuZero,41063.25,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,jamesbond
DreamerV2,40445,False,Mastering Atari with Discrete World Models,2020,jamesbond
IQN,35108,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,jamesbond
MuZero (Res2 Adam),28626.23,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,jamesbond
R2D2,25354.0,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,jamesbond
Ape-X,21322.5,False,Distributed Prioritized Experience Replay,2018,jamesbond
CGP,6130,False,Evolving simple programs for playing Atari games,2018,jamesbond
Prior noop,5148.0,False,Prioritized Experience Replay,2015,jamesbond
QR-DQN-1,4703,False,Distributional Reinforcement Learning with Quantile Regression,2017,jamesbond
Prior hs,3961.0,False,Prioritized Experience Replay,2015,jamesbond
C51 noop,1909.0,False,A Distributional Perspective on Reinforcement Learning,2017,jamesbond
Bootstrapped DQN,1663.5,False,Deep Exploration via Bootstrapped DQN,2016,jamesbond
DDQN (tuned) noop,1358.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,jamesbond
Duel noop,1312.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,jamesbond
Recurrent Rational DQN Average,1137,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,jamesbond
Rational DQN Average,1122,False,Adaptive Rational Activations to Boost Deep Reinforcement Learning,2021,jamesbond
Advantage Learning,848.46,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,jamesbond
Duel hs,835.5,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,jamesbond
Prior+Duel noop,812.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,jamesbond
Persistent AL,772.09,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,jamesbond
DQN noop,768.5,False,Deep Reinforcement Learning with Double Q-learning,2015,jamesbond
DQN hs,697.5,False,Deep Reinforcement Learning with Double Q-learning,2015,jamesbond
A3C LSTM hs,613.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,jamesbond
IMPALA (deep),601.50,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,jamesbond
Prior+Duel hs,585.0,False,Deep Reinforcement Learning with Double Q-learning,2015,jamesbond
Nature DQN,576.7,False,,,jamesbond
DDQN (tuned) hs,573.0,False,Deep Reinforcement Learning with Double Q-learning,2015,jamesbond
A3C FF hs,541.0,False,Asynchronous Methods for Deep Reinforcement Learning,2016,jamesbond
DDQN+Pop-Art noop,507.5,False,Learning values across many orders of magnitude,2016,jamesbond
Gorila,444.0,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,jamesbond
POP3D,358.54,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,jamesbond
SARSA,354.1,False,,,jamesbond
A3C FF (1 day) hs,351.5,False,Asynchronous Methods for Deep Reinforcement Learning,2016,jamesbond
UCT,330,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,jamesbond
A2C + SIL,310.8,False,Self-Imitation Learning,2018,jamesbond
Best Learner,202.8,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,jamesbond
SAC,68.3,False,Soft Actor-Critic for Discrete Action Settings,2019,jamesbond
Go-Explore,107363,False,Go-Explore: a New Approach for Hard-Exploration Problems,2019,pitfall
Agent57,18756.01,False,Agent57: Outperforming the Atari Human Benchmark,2020,pitfall
Go-Explore,6954,False,"First return, then explore",2020,pitfall
IQN,0,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,pitfall
MuZero,0.00,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,pitfall
R2D2,0.0,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,pitfall
CGP,0,False,Evolving simple programs for playing Atari games,2018,pitfall
NoisyNet-Dueling,0,False,Noisy Networks for Exploration,2017,pitfall
POP3D,0,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,pitfall
Advantage Learning,0,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,pitfall
QR-DQN-1,0,False,Distributional Reinforcement Learning with Quantile Regression,2017,pitfall
DreamerV2,0,False,Mastering Atari with Discrete World Models,2020,pitfall
MuZero (Res2 Adam),0,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,pitfall
GDI-I3,0,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,pitfall
RND,-3,False,Exploration by Random Network Distillation,2018,pitfall
Ape-X,-0.6,False,Distributed Prioritized Experience Replay,2018,pitfall
IMPALA (deep),-1.66,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,pitfall
GDI-H3(200M frames),-4.3,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,pitfall
GDI-I3,986440,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,upndown
GDI-H3(200M frames),966590,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,upndown
MuZero,715545.61,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,upndown
DreamerV2,653662,False,Mastering Atari with Discrete World Models,2020,upndown
MuZero (Res2 Adam),634898.18,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,upndown
Agent57,623805.73,False,Agent57: Outperforming the Atari Human Benchmark,2020,upndown
R2D2,589226.9,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,upndown
Ape-X,401884.3,False,Distributed Prioritized Experience Replay,2018,upndown
RIMs-PPO,390000,False,,,upndown
IMPALA (deep),332546.75,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,upndown
POP3D,242701.51,False,Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization,2018,upndown
A3C LSTM hs,105728.7,False,Asynchronous Methods for Deep Reinforcement Learning,2016,upndown
IQN,88148,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,upndown
A3C FF hs,74705.7,False,Asynchronous Methods for Deep Reinforcement Learning,2016,upndown
UCT,74473.6,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,upndown
QR-DQN-1,71260,False,Distributional Reinforcement Learning with Quantile Regression,2017,upndown
ES FF (1 hour) noop,67974.0,False,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017,upndown
NoisyNet-Dueling,61326,False,Noisy Networks for Exploration,2017,upndown
A3C FF (1 day) hs,54525.4,False,Asynchronous Methods for Deep Reinforcement Learning,2016,upndown
A2C + SIL,53314.6,False,Self-Imitation Learning,2018,upndown
Duel noop,44939.6,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,upndown
Prior+Duel noop,33879.1,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,upndown
Bootstrapped DQN,26231,False,Deep Exploration via Bootstrapped DQN,2016,upndown
Duel hs,24759.2,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,upndown
DDQN (tuned) noop,22972.2,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,upndown
Prior+Duel hs,22681.3,False,Deep Reinforcement Learning with Double Q-learning,2015,upndown
DDQN+Pop-Art noop,22474.4,False,Learning values across many orders of magnitude,2016,upndown
DDQN (tuned) hs,19086.9,False,Deep Reinforcement Learning with Double Q-learning,2015,upndown
Prior noop,16154.1,False,Prioritized Experience Replay,2015,upndown
C51 noop,15612.0,False,A Distributional Perspective on Reinforcement Learning,2017,upndown
CGP,14524,False,Evolving simple programs for playing Atari games,2018,upndown
Advantage Learning,13909.74,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,upndown
Prior hs,12157.4,False,Prioritized Experience Replay,2015,upndown
DQN noop,9989.9,False,Deep Reinforcement Learning with Double Q-learning,2015,upndown
Gorila,8747.7,False,Massively Parallel Methods for Deep Reinforcement Learning,2015,upndown
Nature DQN,8456.0,False,,,upndown
DQN hs,8038.5,False,Deep Reinforcement Learning with Double Q-learning,2015,upndown
Best Learner,3532.7,False,The Arcade Learning Environment: An Evaluation Platform for General Agents,2012,upndown
CURL,2735.2,False,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,2020,upndown
SARSA,2449.0,False,,,upndown
SAC,250.7,False,Soft Actor-Critic for Discrete Action Settings,2019,upndown
GDI-H3(200M frames),959580,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,phoenix
MuZero,955137.84,False,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,phoenix
Agent57,908264.15,False,Agent57: Outperforming the Atari Human Benchmark,2020,phoenix
GDI-I3,894460,False,GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning,2021,phoenix
R2D2,864020.0,False,Recurrent Experience Replay in Distributed Reinforcement Learning,2019,phoenix
MuZero (Res2 Adam),815728.7,False,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021,phoenix
Ape-X,224491.1,False,Distributed Prioritized Experience Replay,2018,phoenix
IMPALA (deep),210996.45,False,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,2018,phoenix
FQF,174077.5,False,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,2019,phoenix
Prior+Duel hs,63597.0,False,Dueling Network Architectures for Deep Reinforcement Learning,2015,phoenix
IQN,56599,False,Implicit Quantile Networks for Distributional Reinforcement Learning,2018,phoenix
DreamerV2,49375,False,Mastering Atari with Discrete World Models,2020,phoenix
Advantage Learning,22038.27,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,phoenix
QR-DQN-1,16585,False,Distributional Reinforcement Learning with Quantile Regression,2017,phoenix
Persistent AL,14495.56,False,Increasing the Action Gap: New Operators for Reinforcement Learning,2015,phoenix
NoisyNet-Dueling,10379,False,Noisy Networks for Exploration,2017,phoenix
CGP,7520,False,Evolving simple programs for playing Atari games,2018,phoenix
IDVQ + DRSC + XNES,4600,False,Playing Atari with Six Neurons,2018,phoenix
